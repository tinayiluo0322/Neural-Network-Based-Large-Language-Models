{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8E4u+XgoSVzQtA94z7ik1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinayiluo0322/Neural-Network-Based-Large-Language-Models/blob/main/MNIST_Classification_Pytorch_MLP_LR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Data Classification\n",
        "\n",
        "###### Luopeiwen Yi"
      ],
      "metadata": {
        "id": "TXqIXoIkAGVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import struct\n",
        "import os\n",
        "import urllib.request\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VV5lF7KHBv-N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUtnQH7Q03U8",
        "outputId": "45c1839d-c935-4de4-bd28-5c58779e572e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary Classification 4 and 9"
      ],
      "metadata": {
        "id": "td2uGu69ADLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset without transformation (raw data)\n",
        "raw_train_dataset = torchvision.datasets.MNIST(root=\"./mnist_data\", train=True, download=True)\n",
        "raw_test_dataset = torchvision.datasets.MNIST(root=\"./mnist_data\", train=False, download=True)\n",
        "\n",
        "# Function to filter only digits \"4\" and \"9\" before applying transformations\n",
        "def filter_digits_raw(dataset, digit1=9, digit2=4):\n",
        "    mask = (dataset.targets == digit1) | (dataset.targets == digit2)  # Filter digits 9 and 4\n",
        "    filtered_images = dataset.data[mask].float()  # Keep raw values (0-255), normalization happens later\n",
        "    filtered_labels = torch.where(dataset.targets[mask] == digit1, 0, 1)  # Binary labels (0 for '9', 1 for '4')\n",
        "    return filtered_images, filtered_labels\n",
        "\n",
        "# Filter datasets for digits 4 & 9 before transformation\n",
        "train_images, train_labels = filter_digits_raw(raw_train_dataset)\n",
        "test_images, test_labels = filter_digits_raw(raw_test_dataset)\n",
        "\n",
        "# Compute mean & std from the training dataset\n",
        "train_mean = train_images.mean().item() / 255.0  # Compute in [0,1] range\n",
        "train_std = train_images.std().item() / 255.0\n",
        "print(f\"Computed Train Set Mean: {train_mean:.4f}, Train Set Std: {train_std:.4f}\")\n",
        "\n",
        "# Define final transformation using the computed mean & std\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to tensor, scales to [0, 1]\n",
        "    transforms.Normalize((train_mean,), (train_std,))  # Apply computed train set stats\n",
        "])\n",
        "\n",
        "# Load the dataset with the correct transformation\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./mnist_data\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./mnist_data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Function to filter only digits \"4\" and \"9\" after applying transformations\n",
        "def filter_digits(dataset, digit1=9, digit2=4):\n",
        "    mask = (dataset.targets == digit1) | (dataset.targets == digit2)\n",
        "    filtered_images = dataset.data[mask]  # No need to convert to float again, already normalized\n",
        "    filtered_labels = torch.where(dataset.targets[mask] == digit1, 0, 1)\n",
        "    return filtered_images, filtered_labels\n",
        "\n",
        "# Filter the datasets again after applying the transformation\n",
        "train_images, train_labels = filter_digits(train_dataset)\n",
        "test_images, test_labels = filter_digits(test_dataset)\n",
        "\n",
        "# Reshape images for ML models (Flatten 28x28 into 784)\n",
        "train_images = train_images.view(train_images.shape[0], -1).to(device)\n",
        "test_images = test_images.view(test_images.shape[0], -1).to(device)\n",
        "train_labels = train_labels.to(device)\n",
        "test_labels = test_labels.to(device)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f\"Train set: {len(train_images)}, Labels: {len(train_labels)}\")\n",
        "print(f\"Test set: {len(test_images)}, Labels: {len(test_labels)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G9NIqRYBNOZ",
        "outputId": "1d6db6f4-d9c3-4991-dbb4-d4b7019579b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed Train Set Mean: 0.1220, Train Set Std: 0.2981\n",
            "Train set: 11791, Labels: 11791\n",
            "Test set: 1991, Labels: 1991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Regression Binary Classification"
      ],
      "metadata": {
        "id": "xsREv5Z6Wdln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)  # Linear layer for w^T x + b\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))  # Apply sigmoid function"
      ],
      "metadata": {
        "id": "K6aTdEJFHEWe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "input_dim = 28 * 28  # MNIST images are 28x28 pixels\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 25"
      ],
      "metadata": {
        "id": "kt_X7892IbEb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract tensors from Subset objects and move to device\n",
        "train_images_tensor = torch.stack([train_images[i] for i in range(len(train_images))]).to(device).float()\n",
        "train_labels_tensor = torch.stack([train_labels[i] for i in range(len(train_labels))]).to(device)\n",
        "\n",
        "test_images_tensor = torch.stack([test_images[i] for i in range(len(test_images))]).to(device).float()\n",
        "test_labels_tensor = torch.stack([test_labels[i] for i in range(len(test_labels))]).to(device)\n",
        "\n",
        "# Convert to TensorDataset\n",
        "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor.unsqueeze(1).float())\n",
        "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor.unsqueeze(1).float())\n",
        "\n",
        "# Create DataLoaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "xMO53WqQ0rT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = LogisticRegression(input_dim).to(device)\n",
        "criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss for classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_ims = 0\n",
        "    total_batches = 0\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        total_ims += labels.shape[0]\n",
        "        total_batches += 1\n",
        "        total_loss += loss.item()\n",
        "        total_corrects += ((outputs >= 0.5).float() == labels).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    acc = (total_corrects / total_ims) * 100.0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emheto1OHKkZ",
        "outputId": "e275e953-4d3c-4508-8150-f84152e5f5b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 28.8760\n",
            "Batch 100: Loss = 0.9117\n",
            "Batch 200: Loss = 0.0005\n",
            "Batch 300: Loss = 0.0236\n",
            "Epoch 1/25, Training Loss: 4.9127, Training Accuracy: 88.84%\n",
            "Batch 0: Loss = 3.1885\n",
            "Batch 100: Loss = 0.0012\n",
            "Batch 200: Loss = 0.0696\n",
            "Batch 300: Loss = 0.5887\n",
            "Epoch 2/25, Training Loss: 1.5310, Training Accuracy: 95.24%\n",
            "Batch 0: Loss = 13.7183\n",
            "Batch 100: Loss = 0.7847\n",
            "Batch 200: Loss = 0.0000\n",
            "Batch 300: Loss = 0.1049\n",
            "Epoch 3/25, Training Loss: 1.4360, Training Accuracy: 95.59%\n",
            "Batch 0: Loss = 0.2507\n",
            "Batch 100: Loss = 0.1798\n",
            "Batch 200: Loss = 0.6808\n",
            "Batch 300: Loss = 0.4906\n",
            "Epoch 4/25, Training Loss: 1.3002, Training Accuracy: 95.79%\n",
            "Batch 0: Loss = 3.6727\n",
            "Batch 100: Loss = 0.0754\n",
            "Batch 200: Loss = 0.3339\n",
            "Batch 300: Loss = 3.6874\n",
            "Epoch 5/25, Training Loss: 1.1242, Training Accuracy: 95.99%\n",
            "Batch 0: Loss = 0.1164\n",
            "Batch 100: Loss = 0.7612\n",
            "Batch 200: Loss = 0.0014\n",
            "Batch 300: Loss = 3.1250\n",
            "Epoch 6/25, Training Loss: 1.0999, Training Accuracy: 96.16%\n",
            "Batch 0: Loss = 0.2110\n",
            "Batch 100: Loss = 3.1250\n",
            "Batch 200: Loss = 0.9092\n",
            "Batch 300: Loss = 7.0152\n",
            "Epoch 7/25, Training Loss: 1.1265, Training Accuracy: 96.01%\n",
            "Batch 0: Loss = 1.0093\n",
            "Batch 100: Loss = 0.1676\n",
            "Batch 200: Loss = 1.1831\n",
            "Batch 300: Loss = 0.0000\n",
            "Epoch 8/25, Training Loss: 1.0441, Training Accuracy: 96.38%\n",
            "Batch 0: Loss = 0.6440\n",
            "Batch 100: Loss = 0.7650\n",
            "Batch 200: Loss = 3.3390\n",
            "Batch 300: Loss = 0.9146\n",
            "Epoch 9/25, Training Loss: 0.9634, Training Accuracy: 96.34%\n",
            "Batch 0: Loss = 0.0075\n",
            "Batch 100: Loss = 3.1250\n",
            "Batch 200: Loss = 0.0000\n",
            "Batch 300: Loss = 0.0003\n",
            "Epoch 10/25, Training Loss: 0.8784, Training Accuracy: 96.60%\n",
            "Batch 0: Loss = 0.9783\n",
            "Batch 100: Loss = 0.6344\n",
            "Batch 200: Loss = 0.2635\n",
            "Batch 300: Loss = 0.3953\n",
            "Epoch 11/25, Training Loss: 0.9522, Training Accuracy: 96.24%\n",
            "Batch 0: Loss = 0.3017\n",
            "Batch 100: Loss = 0.0147\n",
            "Batch 200: Loss = 0.0206\n",
            "Batch 300: Loss = 1.4643\n",
            "Epoch 12/25, Training Loss: 0.9262, Training Accuracy: 96.50%\n",
            "Batch 0: Loss = 0.0056\n",
            "Batch 100: Loss = 0.4500\n",
            "Batch 200: Loss = 0.8297\n",
            "Batch 300: Loss = 0.0787\n",
            "Epoch 13/25, Training Loss: 0.9368, Training Accuracy: 96.57%\n",
            "Batch 0: Loss = 3.6015\n",
            "Batch 100: Loss = 0.0001\n",
            "Batch 200: Loss = 0.0000\n",
            "Batch 300: Loss = 3.1250\n",
            "Epoch 14/25, Training Loss: 0.8735, Training Accuracy: 96.45%\n",
            "Batch 0: Loss = 0.0002\n",
            "Batch 100: Loss = 0.0000\n",
            "Batch 200: Loss = 0.0000\n",
            "Batch 300: Loss = 0.3786\n",
            "Epoch 15/25, Training Loss: 0.7890, Training Accuracy: 96.71%\n",
            "Batch 0: Loss = 4.6596\n",
            "Batch 100: Loss = 3.8425\n",
            "Batch 200: Loss = 3.1250\n",
            "Batch 300: Loss = 0.3542\n",
            "Epoch 16/25, Training Loss: 0.7885, Training Accuracy: 96.49%\n",
            "Batch 0: Loss = 0.0460\n",
            "Batch 100: Loss = 1.0111\n",
            "Batch 200: Loss = 3.1291\n",
            "Batch 300: Loss = 3.1250\n",
            "Epoch 17/25, Training Loss: 0.8348, Training Accuracy: 96.60%\n",
            "Batch 0: Loss = 0.0000\n",
            "Batch 100: Loss = 3.2940\n",
            "Batch 200: Loss = 0.0121\n",
            "Batch 300: Loss = 0.1122\n",
            "Epoch 18/25, Training Loss: 0.7331, Training Accuracy: 96.74%\n",
            "Batch 0: Loss = 0.0277\n",
            "Batch 100: Loss = 0.3601\n",
            "Batch 200: Loss = 0.1014\n",
            "Batch 300: Loss = 0.6688\n",
            "Epoch 19/25, Training Loss: 0.8113, Training Accuracy: 96.52%\n",
            "Batch 0: Loss = 3.1250\n",
            "Batch 100: Loss = 0.0000\n",
            "Batch 200: Loss = 4.5460\n",
            "Batch 300: Loss = 0.2462\n",
            "Epoch 20/25, Training Loss: 0.7562, Training Accuracy: 96.88%\n",
            "Batch 0: Loss = 0.8241\n",
            "Batch 100: Loss = 0.3019\n",
            "Batch 200: Loss = 0.0318\n",
            "Batch 300: Loss = 3.1614\n",
            "Epoch 21/25, Training Loss: 0.7466, Training Accuracy: 96.60%\n",
            "Batch 0: Loss = 0.0109\n",
            "Batch 100: Loss = 3.1350\n",
            "Batch 200: Loss = 0.0000\n",
            "Batch 300: Loss = 0.0026\n",
            "Epoch 22/25, Training Loss: 0.7704, Training Accuracy: 96.66%\n",
            "Batch 0: Loss = 0.3495\n",
            "Batch 100: Loss = 3.2872\n",
            "Batch 200: Loss = 0.2091\n",
            "Batch 300: Loss = 0.4467\n",
            "Epoch 23/25, Training Loss: 0.7156, Training Accuracy: 96.71%\n",
            "Batch 0: Loss = 0.0002\n",
            "Batch 100: Loss = 0.1706\n",
            "Batch 200: Loss = 3.1250\n",
            "Batch 300: Loss = 3.1294\n",
            "Epoch 24/25, Training Loss: 0.7849, Training Accuracy: 96.93%\n",
            "Batch 0: Loss = 3.3003\n",
            "Batch 100: Loss = 3.1252\n",
            "Batch 200: Loss = 3.5192\n",
            "Batch 300: Loss = 0.1131\n",
            "Epoch 25/25, Training Loss: 0.7287, Training Accuracy: 96.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model on the test set\n",
        "def test_model(model, test_loader, criterion, device):\n",
        "    total_ims = 0\n",
        "    total_batches = 0\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Do NOT compute gradients\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            # Move data to GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Convert outputs to binary predictions\n",
        "            predictions = (outputs >= 0.5).float()\n",
        "\n",
        "            # Count correct predictions\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            # Accumulate totals\n",
        "            total_ims += labels.shape[0]\n",
        "            total_batches += 1\n",
        "            total_loss += loss.item()\n",
        "            total_corrects += correct\n",
        "\n",
        "    # Compute average loss and accuracy\n",
        "    avg_loss = total_loss / total_batches\n",
        "    acc = (total_corrects / total_ims) * 100.0\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "# Call test function\n",
        "test_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THqyyP564kl0",
        "outputId": "a5de3fec-bc15-47e5-9e76-b4409c34a9d7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6724, Test Accuracy: 96.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract learned weights and reshape them into an image (28x28)\n",
        "learned_weights = model.linear.weight[0].data.view(28, 28).cpu().numpy()\n",
        "\n",
        "# Plot learned weights as an image\n",
        "plt.imshow(learned_weights, cmap=\"gray\")\n",
        "plt.colorbar()\n",
        "plt.title(\"Learned Weight Vector (w) as Image\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "eS6wcYd3w18q",
        "outputId": "3ccfd0ba-5041-49f8-8fc2-0e83d1bd36bb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilayer Perceptron Binary Classification"
      ],
      "metadata": {
        "id": "K6m4V7B2recq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Multilayer Perceptron model (MLP)\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, d1, d2):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_dim, d1)  # First hidden layer\n",
        "        self.hidden2 = nn.Linear(d1, d2)  # Second hidden layer\n",
        "        self.output = nn.Linear(d2, 1)  # Output layer (single neuron for binary classification)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.hidden2(x))  # Apply ReLU activation\n",
        "        x = torch.sigmoid(self.output(x))  # Sigmoid for binary classification\n",
        "        return x"
      ],
      "metadata": {
        "id": "uiTHluOptbQ7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_dim = 28 * 28  # MNIST images are 28x28 pixels\n",
        "d1 = 128  # First hidden layer size\n",
        "d2 = 64   # Second hidden layer size\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "Wq3weXByteJl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = MLP(input_dim, d1, d2).to(device)\n",
        "criterion = nn.BCELoss().to(device)  # Binary Cross-Entropy Loss for classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_ims = 0\n",
        "    total_batches = 0\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        total_ims += labels.shape[0]\n",
        "        total_batches += 1\n",
        "        total_loss += loss.item()\n",
        "        total_corrects += ((outputs >= 0.5).float() == labels).sum().item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    acc = (total_corrects / total_ims) * 100.0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLP3SeCErghe",
        "outputId": "fd6d5457-7931-44f1-eb0b-ecbcbac617b8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 6.5505\n",
            "Batch 100: Loss = 0.3988\n",
            "Batch 200: Loss = 0.2170\n",
            "Batch 300: Loss = 0.2814\n",
            "Epoch 1/50, Training Loss: 0.3677, Training Accuracy: 87.56%\n",
            "Batch 0: Loss = 0.0882\n",
            "Batch 100: Loss = 0.1552\n",
            "Batch 200: Loss = 0.2957\n",
            "Batch 300: Loss = 0.0812\n",
            "Epoch 2/50, Training Loss: 0.1427, Training Accuracy: 95.01%\n",
            "Batch 0: Loss = 0.0918\n",
            "Batch 100: Loss = 0.0811\n",
            "Batch 200: Loss = 0.0171\n",
            "Batch 300: Loss = 0.0628\n",
            "Epoch 3/50, Training Loss: 0.1115, Training Accuracy: 96.00%\n",
            "Batch 0: Loss = 0.0851\n",
            "Batch 100: Loss = 0.0822\n",
            "Batch 200: Loss = 0.0305\n",
            "Batch 300: Loss = 0.1275\n",
            "Epoch 4/50, Training Loss: 0.0940, Training Accuracy: 96.57%\n",
            "Batch 0: Loss = 0.0488\n",
            "Batch 100: Loss = 0.0427\n",
            "Batch 200: Loss = 0.0064\n",
            "Batch 300: Loss = 0.1068\n",
            "Epoch 5/50, Training Loss: 0.0827, Training Accuracy: 96.99%\n",
            "Batch 0: Loss = 0.0386\n",
            "Batch 100: Loss = 0.0527\n",
            "Batch 200: Loss = 0.1613\n",
            "Batch 300: Loss = 0.0056\n",
            "Epoch 6/50, Training Loss: 0.0742, Training Accuracy: 97.40%\n",
            "Batch 0: Loss = 0.0369\n",
            "Batch 100: Loss = 0.0391\n",
            "Batch 200: Loss = 0.0160\n",
            "Batch 300: Loss = 0.0676\n",
            "Epoch 7/50, Training Loss: 0.0679, Training Accuracy: 97.57%\n",
            "Batch 0: Loss = 0.0833\n",
            "Batch 100: Loss = 0.0547\n",
            "Batch 200: Loss = 0.0872\n",
            "Batch 300: Loss = 0.0341\n",
            "Epoch 8/50, Training Loss: 0.0624, Training Accuracy: 97.72%\n",
            "Batch 0: Loss = 0.0804\n",
            "Batch 100: Loss = 0.0717\n",
            "Batch 200: Loss = 0.0456\n",
            "Batch 300: Loss = 0.0310\n",
            "Epoch 9/50, Training Loss: 0.0579, Training Accuracy: 97.97%\n",
            "Batch 0: Loss = 0.1051\n",
            "Batch 100: Loss = 0.2273\n",
            "Batch 200: Loss = 0.0909\n",
            "Batch 300: Loss = 0.0018\n",
            "Epoch 10/50, Training Loss: 0.0540, Training Accuracy: 98.08%\n",
            "Batch 0: Loss = 0.1073\n",
            "Batch 100: Loss = 0.0045\n",
            "Batch 200: Loss = 0.0140\n",
            "Batch 300: Loss = 0.1141\n",
            "Epoch 11/50, Training Loss: 0.0508, Training Accuracy: 98.29%\n",
            "Batch 0: Loss = 0.1706\n",
            "Batch 100: Loss = 0.0662\n",
            "Batch 200: Loss = 0.0280\n",
            "Batch 300: Loss = 0.0422\n",
            "Epoch 12/50, Training Loss: 0.0476, Training Accuracy: 98.43%\n",
            "Batch 0: Loss = 0.0088\n",
            "Batch 100: Loss = 0.0559\n",
            "Batch 200: Loss = 0.0289\n",
            "Batch 300: Loss = 0.0249\n",
            "Epoch 13/50, Training Loss: 0.0451, Training Accuracy: 98.51%\n",
            "Batch 0: Loss = 0.0793\n",
            "Batch 100: Loss = 0.1563\n",
            "Batch 200: Loss = 0.0234\n",
            "Batch 300: Loss = 0.0067\n",
            "Epoch 14/50, Training Loss: 0.0429, Training Accuracy: 98.59%\n",
            "Batch 0: Loss = 0.1447\n",
            "Batch 100: Loss = 0.0417\n",
            "Batch 200: Loss = 0.0142\n",
            "Batch 300: Loss = 0.0020\n",
            "Epoch 15/50, Training Loss: 0.0411, Training Accuracy: 98.61%\n",
            "Batch 0: Loss = 0.0101\n",
            "Batch 100: Loss = 0.0632\n",
            "Batch 200: Loss = 0.0095\n",
            "Batch 300: Loss = 0.0324\n",
            "Epoch 16/50, Training Loss: 0.0387, Training Accuracy: 98.78%\n",
            "Batch 0: Loss = 0.0218\n",
            "Batch 100: Loss = 0.0167\n",
            "Batch 200: Loss = 0.3109\n",
            "Batch 300: Loss = 0.0204\n",
            "Epoch 17/50, Training Loss: 0.0368, Training Accuracy: 98.90%\n",
            "Batch 0: Loss = 0.0367\n",
            "Batch 100: Loss = 0.0135\n",
            "Batch 200: Loss = 0.0410\n",
            "Batch 300: Loss = 0.0209\n",
            "Epoch 18/50, Training Loss: 0.0351, Training Accuracy: 98.91%\n",
            "Batch 0: Loss = 0.1138\n",
            "Batch 100: Loss = 0.0266\n",
            "Batch 200: Loss = 0.0554\n",
            "Batch 300: Loss = 0.0555\n",
            "Epoch 19/50, Training Loss: 0.0335, Training Accuracy: 99.02%\n",
            "Batch 0: Loss = 0.0068\n",
            "Batch 100: Loss = 0.0539\n",
            "Batch 200: Loss = 0.0300\n",
            "Batch 300: Loss = 0.0384\n",
            "Epoch 20/50, Training Loss: 0.0323, Training Accuracy: 99.00%\n",
            "Batch 0: Loss = 0.0092\n",
            "Batch 100: Loss = 0.0349\n",
            "Batch 200: Loss = 0.0177\n",
            "Batch 300: Loss = 0.0161\n",
            "Epoch 21/50, Training Loss: 0.0319, Training Accuracy: 99.12%\n",
            "Batch 0: Loss = 0.0059\n",
            "Batch 100: Loss = 0.0562\n",
            "Batch 200: Loss = 0.0103\n",
            "Batch 300: Loss = 0.2157\n",
            "Epoch 22/50, Training Loss: 0.0299, Training Accuracy: 99.21%\n",
            "Batch 0: Loss = 0.0102\n",
            "Batch 100: Loss = 0.0189\n",
            "Batch 200: Loss = 0.0050\n",
            "Batch 300: Loss = 0.0281\n",
            "Epoch 23/50, Training Loss: 0.0287, Training Accuracy: 99.20%\n",
            "Batch 0: Loss = 0.0439\n",
            "Batch 100: Loss = 0.0063\n",
            "Batch 200: Loss = 0.0494\n",
            "Batch 300: Loss = 0.0364\n",
            "Epoch 24/50, Training Loss: 0.0277, Training Accuracy: 99.28%\n",
            "Batch 0: Loss = 0.0342\n",
            "Batch 100: Loss = 0.0105\n",
            "Batch 200: Loss = 0.0071\n",
            "Batch 300: Loss = 0.1603\n",
            "Epoch 25/50, Training Loss: 0.0264, Training Accuracy: 99.30%\n",
            "Batch 0: Loss = 0.0376\n",
            "Batch 100: Loss = 0.0136\n",
            "Batch 200: Loss = 0.0192\n",
            "Batch 300: Loss = 0.0019\n",
            "Epoch 26/50, Training Loss: 0.0257, Training Accuracy: 99.36%\n",
            "Batch 0: Loss = 0.0148\n",
            "Batch 100: Loss = 0.0577\n",
            "Batch 200: Loss = 0.0149\n",
            "Batch 300: Loss = 0.1692\n",
            "Epoch 27/50, Training Loss: 0.0246, Training Accuracy: 99.36%\n",
            "Batch 0: Loss = 0.0244\n",
            "Batch 100: Loss = 0.0095\n",
            "Batch 200: Loss = 0.0160\n",
            "Batch 300: Loss = 0.0347\n",
            "Epoch 28/50, Training Loss: 0.0241, Training Accuracy: 99.40%\n",
            "Batch 0: Loss = 0.0251\n",
            "Batch 100: Loss = 0.0093\n",
            "Batch 200: Loss = 0.0160\n",
            "Batch 300: Loss = 0.0106\n",
            "Epoch 29/50, Training Loss: 0.0231, Training Accuracy: 99.46%\n",
            "Batch 0: Loss = 0.0152\n",
            "Batch 100: Loss = 0.0189\n",
            "Batch 200: Loss = 0.0029\n",
            "Batch 300: Loss = 0.0430\n",
            "Epoch 30/50, Training Loss: 0.0227, Training Accuracy: 99.46%\n",
            "Batch 0: Loss = 0.0658\n",
            "Batch 100: Loss = 0.0224\n",
            "Batch 200: Loss = 0.0050\n",
            "Batch 300: Loss = 0.0142\n",
            "Epoch 31/50, Training Loss: 0.0218, Training Accuracy: 99.45%\n",
            "Batch 0: Loss = 0.0134\n",
            "Batch 100: Loss = 0.0048\n",
            "Batch 200: Loss = 0.0133\n",
            "Batch 300: Loss = 0.0458\n",
            "Epoch 32/50, Training Loss: 0.0210, Training Accuracy: 99.45%\n",
            "Batch 0: Loss = 0.0017\n",
            "Batch 100: Loss = 0.0197\n",
            "Batch 200: Loss = 0.0464\n",
            "Batch 300: Loss = 0.0056\n",
            "Epoch 33/50, Training Loss: 0.0204, Training Accuracy: 99.53%\n",
            "Batch 0: Loss = 0.0388\n",
            "Batch 100: Loss = 0.0016\n",
            "Batch 200: Loss = 0.0249\n",
            "Batch 300: Loss = 0.0313\n",
            "Epoch 34/50, Training Loss: 0.0197, Training Accuracy: 99.53%\n",
            "Batch 0: Loss = 0.0032\n",
            "Batch 100: Loss = 0.0389\n",
            "Batch 200: Loss = 0.0316\n",
            "Batch 300: Loss = 0.0047\n",
            "Epoch 35/50, Training Loss: 0.0190, Training Accuracy: 99.56%\n",
            "Batch 0: Loss = 0.0060\n",
            "Batch 100: Loss = 0.0286\n",
            "Batch 200: Loss = 0.0931\n",
            "Batch 300: Loss = 0.0086\n",
            "Epoch 36/50, Training Loss: 0.0187, Training Accuracy: 99.61%\n",
            "Batch 0: Loss = 0.0051\n",
            "Batch 100: Loss = 0.0111\n",
            "Batch 200: Loss = 0.0237\n",
            "Batch 300: Loss = 0.0186\n",
            "Epoch 37/50, Training Loss: 0.0180, Training Accuracy: 99.69%\n",
            "Batch 0: Loss = 0.0091\n",
            "Batch 100: Loss = 0.0032\n",
            "Batch 200: Loss = 0.0119\n",
            "Batch 300: Loss = 0.0064\n",
            "Epoch 38/50, Training Loss: 0.0175, Training Accuracy: 99.62%\n",
            "Batch 0: Loss = 0.2060\n",
            "Batch 100: Loss = 0.0300\n",
            "Batch 200: Loss = 0.0147\n",
            "Batch 300: Loss = 0.0032\n",
            "Epoch 39/50, Training Loss: 0.0169, Training Accuracy: 99.66%\n",
            "Batch 0: Loss = 0.0026\n",
            "Batch 100: Loss = 0.0129\n",
            "Batch 200: Loss = 0.0044\n",
            "Batch 300: Loss = 0.0036\n",
            "Epoch 40/50, Training Loss: 0.0166, Training Accuracy: 99.68%\n",
            "Batch 0: Loss = 0.0048\n",
            "Batch 100: Loss = 0.0200\n",
            "Batch 200: Loss = 0.0684\n",
            "Batch 300: Loss = 0.0037\n",
            "Epoch 41/50, Training Loss: 0.0161, Training Accuracy: 99.68%\n",
            "Batch 0: Loss = 0.0047\n",
            "Batch 100: Loss = 0.0077\n",
            "Batch 200: Loss = 0.0184\n",
            "Batch 300: Loss = 0.0280\n",
            "Epoch 42/50, Training Loss: 0.0156, Training Accuracy: 99.71%\n",
            "Batch 0: Loss = 0.0064\n",
            "Batch 100: Loss = 0.0096\n",
            "Batch 200: Loss = 0.0314\n",
            "Batch 300: Loss = 0.0200\n",
            "Epoch 43/50, Training Loss: 0.0152, Training Accuracy: 99.74%\n",
            "Batch 0: Loss = 0.0068\n",
            "Batch 100: Loss = 0.0042\n",
            "Batch 200: Loss = 0.0117\n",
            "Batch 300: Loss = 0.0145\n",
            "Epoch 44/50, Training Loss: 0.0148, Training Accuracy: 99.73%\n",
            "Batch 0: Loss = 0.0112\n",
            "Batch 100: Loss = 0.0066\n",
            "Batch 200: Loss = 0.0060\n",
            "Batch 300: Loss = 0.0120\n",
            "Epoch 45/50, Training Loss: 0.0144, Training Accuracy: 99.75%\n",
            "Batch 0: Loss = 0.0220\n",
            "Batch 100: Loss = 0.0087\n",
            "Batch 200: Loss = 0.0066\n",
            "Batch 300: Loss = 0.0053\n",
            "Epoch 46/50, Training Loss: 0.0139, Training Accuracy: 99.79%\n",
            "Batch 0: Loss = 0.0199\n",
            "Batch 100: Loss = 0.0033\n",
            "Batch 200: Loss = 0.0050\n",
            "Batch 300: Loss = 0.0094\n",
            "Epoch 47/50, Training Loss: 0.0137, Training Accuracy: 99.78%\n",
            "Batch 0: Loss = 0.0046\n",
            "Batch 100: Loss = 0.0040\n",
            "Batch 200: Loss = 0.0023\n",
            "Batch 300: Loss = 0.0102\n",
            "Epoch 48/50, Training Loss: 0.0133, Training Accuracy: 99.79%\n",
            "Batch 0: Loss = 0.0135\n",
            "Batch 100: Loss = 0.0073\n",
            "Batch 200: Loss = 0.0050\n",
            "Batch 300: Loss = 0.0127\n",
            "Epoch 49/50, Training Loss: 0.0129, Training Accuracy: 99.80%\n",
            "Batch 0: Loss = 0.0025\n",
            "Batch 100: Loss = 0.0016\n",
            "Batch 200: Loss = 0.0114\n",
            "Batch 300: Loss = 0.0065\n",
            "Epoch 50/50, Training Loss: 0.0127, Training Accuracy: 99.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test_model(model, test_loader, criterion, device):\n",
        "    total_ims = 0\n",
        "    total_batches = 0\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            predictions = (outputs >= 0.5).float()\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_ims += labels.shape[0]\n",
        "            total_batches += 1\n",
        "            total_loss += loss.item()\n",
        "            total_corrects += correct\n",
        "\n",
        "    avg_loss = total_loss / total_batches\n",
        "    acc = (total_corrects / total_ims) * 100.0\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP01-xqBuWmB",
        "outputId": "3ec2b2a1-1e39-44fc-e686-8e3a2e280fba"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0404, Test Accuracy: 98.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multilayer Perceptron 10 Class Classification"
      ],
      "metadata": {
        "id": "asNGuwvuwiVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 10-Class MLP Model\n",
        "class MLP_MultiClass(nn.Module):\n",
        "    def __init__(self, input_dim, d1, d2, num_classes):\n",
        "        super(MLP_MultiClass, self).__init__()\n",
        "        self.hidden1 = nn.Linear(input_dim, d1)  # First hidden layer\n",
        "        self.hidden2 = nn.Linear(d1, d2)  # Second hidden layer\n",
        "        self.output = nn.Linear(d2, num_classes)  # Output layer with 10 neurons (one per class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.hidden1(x))  # Apply ReLU activation\n",
        "        x = torch.relu(self.hidden2(x))  # Apply ReLU activation\n",
        "        x = self.output(x)  # No softmax here, PyTorch's CrossEntropyLoss applies it automatically\n",
        "        return x"
      ],
      "metadata": {
        "id": "WEGp_vJZz0fE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameters\n",
        "input_dim = 28 * 28  # MNIST images are 28x28 pixels\n",
        "d1 = 128  # First hidden layer size\n",
        "d2 = 64   # Second hidden layer size\n",
        "num_classes = 10  # Now we classify 10 digits (0-9)\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "bN4zxn_E0ayf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing transformations (Normalize using dataset mean & std)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # Standard MNIST normalization\n",
        "])\n",
        "\n",
        "# Load full MNIST dataset (No filtering, we now use all 10 digits)\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./mnist_data\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./mnist_data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Create DataLoaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "6mfEMWZhxB6n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print train and test dataset size\n",
        "print(f\"Train set: {len(train_dataset)}\")\n",
        "print(f\"Test set: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8y8xZ9S6HyW",
        "outputId": "09f9df07-6cdb-42f7-b4c6-f2b8f8ad5efe"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 60000\n",
            "Test set: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = MLP_MultiClass(input_dim, d1, d2, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)  # Cross-Entropy Loss for multi-class classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "R9fIrskJ1WsU"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Flatten images\n",
        "        images = images.view(images.shape[0], -1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        # Compute accuracy\n",
        "        predictions = torch.argmax(outputs, dim=1)  # Get the predicted class\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_corrects += correct\n",
        "        total_samples += labels.shape[0]\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    acc = (total_corrects / total_samples) * 100.0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIpxNJrT1afx",
        "outputId": "f35ecf3a-72f9-4c31-e01f-577532336e9a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 2.3467\n",
            "Batch 100: Loss = 2.3194\n",
            "Batch 200: Loss = 2.2943\n",
            "Batch 300: Loss = 2.3041\n",
            "Batch 400: Loss = 2.3034\n",
            "Batch 500: Loss = 2.3189\n",
            "Batch 600: Loss = 2.2815\n",
            "Batch 700: Loss = 2.3265\n",
            "Batch 800: Loss = 2.2776\n",
            "Batch 900: Loss = 2.3102\n",
            "Batch 1000: Loss = 2.2967\n",
            "Batch 1100: Loss = 2.2894\n",
            "Batch 1200: Loss = 2.2749\n",
            "Batch 1300: Loss = 2.3049\n",
            "Batch 1400: Loss = 2.2770\n",
            "Batch 1500: Loss = 2.2859\n",
            "Batch 1600: Loss = 2.2645\n",
            "Batch 1700: Loss = 2.2973\n",
            "Batch 1800: Loss = 2.2638\n",
            "Epoch 1/50, Training Loss: 2.2974, Training Accuracy: 10.72%\n",
            "Batch 0: Loss = 2.2873\n",
            "Batch 100: Loss = 2.2752\n",
            "Batch 200: Loss = 2.2543\n",
            "Batch 300: Loss = 2.2589\n",
            "Batch 400: Loss = 2.2827\n",
            "Batch 500: Loss = 2.2715\n",
            "Batch 600: Loss = 2.2453\n",
            "Batch 700: Loss = 2.2151\n",
            "Batch 800: Loss = 2.2536\n",
            "Batch 900: Loss = 2.2600\n",
            "Batch 1000: Loss = 2.2686\n",
            "Batch 1100: Loss = 2.2610\n",
            "Batch 1200: Loss = 2.2478\n",
            "Batch 1300: Loss = 2.2671\n",
            "Batch 1400: Loss = 2.2640\n",
            "Batch 1500: Loss = 2.2488\n",
            "Batch 1600: Loss = 2.2395\n",
            "Batch 1700: Loss = 2.2332\n",
            "Batch 1800: Loss = 2.2163\n",
            "Epoch 2/50, Training Loss: 2.2589, Training Accuracy: 15.95%\n",
            "Batch 0: Loss = 2.2389\n",
            "Batch 100: Loss = 2.2511\n",
            "Batch 200: Loss = 2.1951\n",
            "Batch 300: Loss = 2.2424\n",
            "Batch 400: Loss = 2.2334\n",
            "Batch 500: Loss = 2.2223\n",
            "Batch 600: Loss = 2.1854\n",
            "Batch 700: Loss = 2.2293\n",
            "Batch 800: Loss = 2.2213\n",
            "Batch 900: Loss = 2.2022\n",
            "Batch 1000: Loss = 2.1944\n",
            "Batch 1100: Loss = 2.2006\n",
            "Batch 1200: Loss = 2.1705\n",
            "Batch 1300: Loss = 2.1964\n",
            "Batch 1400: Loss = 2.1995\n",
            "Batch 1500: Loss = 2.1740\n",
            "Batch 1600: Loss = 2.1851\n",
            "Batch 1700: Loss = 2.1943\n",
            "Batch 1800: Loss = 2.2246\n",
            "Epoch 3/50, Training Loss: 2.2178, Training Accuracy: 24.91%\n",
            "Batch 0: Loss = 2.1734\n",
            "Batch 100: Loss = 2.2123\n",
            "Batch 200: Loss = 2.2047\n",
            "Batch 300: Loss = 2.2076\n",
            "Batch 400: Loss = 2.1884\n",
            "Batch 500: Loss = 2.1801\n",
            "Batch 600: Loss = 2.1780\n",
            "Batch 700: Loss = 2.1485\n",
            "Batch 800: Loss = 2.1832\n",
            "Batch 900: Loss = 2.1799\n",
            "Batch 1000: Loss = 2.1400\n",
            "Batch 1100: Loss = 2.1732\n",
            "Batch 1200: Loss = 2.1746\n",
            "Batch 1300: Loss = 2.1856\n",
            "Batch 1400: Loss = 2.1743\n",
            "Batch 1500: Loss = 2.1695\n",
            "Batch 1600: Loss = 2.1573\n",
            "Batch 1700: Loss = 2.1461\n",
            "Batch 1800: Loss = 2.1552\n",
            "Epoch 4/50, Training Loss: 2.1683, Training Accuracy: 36.99%\n",
            "Batch 0: Loss = 2.1782\n",
            "Batch 100: Loss = 2.1368\n",
            "Batch 200: Loss = 2.1265\n",
            "Batch 300: Loss = 2.1361\n",
            "Batch 400: Loss = 2.1165\n",
            "Batch 500: Loss = 2.0713\n",
            "Batch 600: Loss = 2.1530\n",
            "Batch 700: Loss = 2.1213\n",
            "Batch 800: Loss = 2.1120\n",
            "Batch 900: Loss = 2.0918\n",
            "Batch 1000: Loss = 2.0964\n",
            "Batch 1100: Loss = 2.1410\n",
            "Batch 1200: Loss = 2.1349\n",
            "Batch 1300: Loss = 2.1108\n",
            "Batch 1400: Loss = 2.0951\n",
            "Batch 1500: Loss = 2.1172\n",
            "Batch 1600: Loss = 2.1164\n",
            "Batch 1700: Loss = 2.0407\n",
            "Batch 1800: Loss = 1.9978\n",
            "Epoch 5/50, Training Loss: 2.1050, Training Accuracy: 47.96%\n",
            "Batch 0: Loss = 2.0989\n",
            "Batch 100: Loss = 2.0780\n",
            "Batch 200: Loss = 2.0699\n",
            "Batch 300: Loss = 2.0883\n",
            "Batch 400: Loss = 2.0491\n",
            "Batch 500: Loss = 2.0158\n",
            "Batch 600: Loss = 2.0027\n",
            "Batch 700: Loss = 1.9877\n",
            "Batch 800: Loss = 2.0854\n",
            "Batch 900: Loss = 2.0433\n",
            "Batch 1000: Loss = 2.0310\n",
            "Batch 1100: Loss = 1.9868\n",
            "Batch 1200: Loss = 2.0323\n",
            "Batch 1300: Loss = 2.0380\n",
            "Batch 1400: Loss = 1.9704\n",
            "Batch 1500: Loss = 2.0537\n",
            "Batch 1600: Loss = 1.9274\n",
            "Batch 1700: Loss = 2.0385\n",
            "Batch 1800: Loss = 2.0464\n",
            "Epoch 6/50, Training Loss: 2.0237, Training Accuracy: 55.59%\n",
            "Batch 0: Loss = 1.9386\n",
            "Batch 100: Loss = 1.9597\n",
            "Batch 200: Loss = 1.8760\n",
            "Batch 300: Loss = 1.9442\n",
            "Batch 400: Loss = 1.8844\n",
            "Batch 500: Loss = 1.9358\n",
            "Batch 600: Loss = 1.9353\n",
            "Batch 700: Loss = 1.9598\n",
            "Batch 800: Loss = 1.9520\n",
            "Batch 900: Loss = 1.9644\n",
            "Batch 1000: Loss = 1.8848\n",
            "Batch 1100: Loss = 1.8305\n",
            "Batch 1200: Loss = 1.9304\n",
            "Batch 1300: Loss = 1.8959\n",
            "Batch 1400: Loss = 1.9215\n",
            "Batch 1500: Loss = 1.8376\n",
            "Batch 1600: Loss = 1.9786\n",
            "Batch 1700: Loss = 1.9600\n",
            "Batch 1800: Loss = 1.8697\n",
            "Epoch 7/50, Training Loss: 1.9204, Training Accuracy: 61.44%\n",
            "Batch 0: Loss = 1.8603\n",
            "Batch 100: Loss = 1.8326\n",
            "Batch 200: Loss = 1.7944\n",
            "Batch 300: Loss = 1.7699\n",
            "Batch 400: Loss = 1.7661\n",
            "Batch 500: Loss = 1.8482\n",
            "Batch 600: Loss = 1.7393\n",
            "Batch 700: Loss = 1.8310\n",
            "Batch 800: Loss = 1.8313\n",
            "Batch 900: Loss = 1.8079\n",
            "Batch 1000: Loss = 1.7890\n",
            "Batch 1100: Loss = 1.7981\n",
            "Batch 1200: Loss = 1.6865\n",
            "Batch 1300: Loss = 1.8006\n",
            "Batch 1400: Loss = 1.7825\n",
            "Batch 1500: Loss = 1.7002\n",
            "Batch 1600: Loss = 1.7355\n",
            "Batch 1700: Loss = 1.9079\n",
            "Batch 1800: Loss = 1.7797\n",
            "Epoch 8/50, Training Loss: 1.7941, Training Accuracy: 66.15%\n",
            "Batch 0: Loss = 1.7920\n",
            "Batch 100: Loss = 1.6973\n",
            "Batch 200: Loss = 1.6457\n",
            "Batch 300: Loss = 1.7881\n",
            "Batch 400: Loss = 1.6683\n",
            "Batch 500: Loss = 1.7610\n",
            "Batch 600: Loss = 1.7922\n",
            "Batch 700: Loss = 1.6734\n",
            "Batch 800: Loss = 1.7717\n",
            "Batch 900: Loss = 1.6589\n",
            "Batch 1000: Loss = 1.8347\n",
            "Batch 1100: Loss = 1.5616\n",
            "Batch 1200: Loss = 1.6409\n",
            "Batch 1300: Loss = 1.6603\n",
            "Batch 1400: Loss = 1.5808\n",
            "Batch 1500: Loss = 1.5863\n",
            "Batch 1600: Loss = 1.6728\n",
            "Batch 1700: Loss = 1.5414\n",
            "Batch 1800: Loss = 1.6517\n",
            "Epoch 9/50, Training Loss: 1.6486, Training Accuracy: 70.56%\n",
            "Batch 0: Loss = 1.4560\n",
            "Batch 100: Loss = 1.5938\n",
            "Batch 200: Loss = 1.5788\n",
            "Batch 300: Loss = 1.4146\n",
            "Batch 400: Loss = 1.6357\n",
            "Batch 500: Loss = 1.3384\n",
            "Batch 600: Loss = 1.5464\n",
            "Batch 700: Loss = 1.4523\n",
            "Batch 800: Loss = 1.4039\n",
            "Batch 900: Loss = 1.4101\n",
            "Batch 1000: Loss = 1.5041\n",
            "Batch 1100: Loss = 1.6088\n",
            "Batch 1200: Loss = 1.4992\n",
            "Batch 1300: Loss = 1.5178\n",
            "Batch 1400: Loss = 1.5204\n",
            "Batch 1500: Loss = 1.4171\n",
            "Batch 1600: Loss = 1.3887\n",
            "Batch 1700: Loss = 1.4497\n",
            "Batch 1800: Loss = 1.4254\n",
            "Epoch 10/50, Training Loss: 1.4920, Training Accuracy: 74.20%\n",
            "Batch 0: Loss = 1.4195\n",
            "Batch 100: Loss = 1.3099\n",
            "Batch 200: Loss = 1.3980\n",
            "Batch 300: Loss = 1.3462\n",
            "Batch 400: Loss = 1.3804\n",
            "Batch 500: Loss = 1.3419\n",
            "Batch 600: Loss = 1.3437\n",
            "Batch 700: Loss = 1.4221\n",
            "Batch 800: Loss = 1.2615\n",
            "Batch 900: Loss = 1.4432\n",
            "Batch 1000: Loss = 1.3954\n",
            "Batch 1100: Loss = 1.4357\n",
            "Batch 1200: Loss = 1.2891\n",
            "Batch 1300: Loss = 1.3520\n",
            "Batch 1400: Loss = 1.3119\n",
            "Batch 1500: Loss = 1.2975\n",
            "Batch 1600: Loss = 1.1232\n",
            "Batch 1700: Loss = 1.1985\n",
            "Batch 1800: Loss = 1.3546\n",
            "Epoch 11/50, Training Loss: 1.3354, Training Accuracy: 76.81%\n",
            "Batch 0: Loss = 1.3505\n",
            "Batch 100: Loss = 1.3452\n",
            "Batch 200: Loss = 1.2593\n",
            "Batch 300: Loss = 1.2915\n",
            "Batch 400: Loss = 1.2921\n",
            "Batch 500: Loss = 1.0913\n",
            "Batch 600: Loss = 1.2015\n",
            "Batch 700: Loss = 1.1897\n",
            "Batch 800: Loss = 1.1933\n",
            "Batch 900: Loss = 1.2747\n",
            "Batch 1000: Loss = 1.2494\n",
            "Batch 1100: Loss = 1.2294\n",
            "Batch 1200: Loss = 1.2978\n",
            "Batch 1300: Loss = 1.0765\n",
            "Batch 1400: Loss = 1.0361\n",
            "Batch 1500: Loss = 1.1667\n",
            "Batch 1600: Loss = 1.0426\n",
            "Batch 1700: Loss = 1.1910\n",
            "Batch 1800: Loss = 1.1546\n",
            "Epoch 12/50, Training Loss: 1.1904, Training Accuracy: 78.84%\n",
            "Batch 0: Loss = 1.1086\n",
            "Batch 100: Loss = 1.1501\n",
            "Batch 200: Loss = 1.1990\n",
            "Batch 300: Loss = 1.0429\n",
            "Batch 400: Loss = 1.0000\n",
            "Batch 500: Loss = 0.9509\n",
            "Batch 600: Loss = 0.9934\n",
            "Batch 700: Loss = 1.1181\n",
            "Batch 800: Loss = 0.8759\n",
            "Batch 900: Loss = 1.1876\n",
            "Batch 1000: Loss = 0.9074\n",
            "Batch 1100: Loss = 1.0710\n",
            "Batch 1200: Loss = 0.9442\n",
            "Batch 1300: Loss = 1.0724\n",
            "Batch 1400: Loss = 1.2574\n",
            "Batch 1500: Loss = 1.1068\n",
            "Batch 1600: Loss = 0.9068\n",
            "Batch 1700: Loss = 1.0983\n",
            "Batch 1800: Loss = 0.9344\n",
            "Epoch 13/50, Training Loss: 1.0649, Training Accuracy: 80.22%\n",
            "Batch 0: Loss = 0.8761\n",
            "Batch 100: Loss = 1.0058\n",
            "Batch 200: Loss = 1.0586\n",
            "Batch 300: Loss = 1.0522\n",
            "Batch 400: Loss = 1.0924\n",
            "Batch 500: Loss = 0.9887\n",
            "Batch 600: Loss = 1.0425\n",
            "Batch 700: Loss = 0.7635\n",
            "Batch 800: Loss = 0.8838\n",
            "Batch 900: Loss = 0.9317\n",
            "Batch 1000: Loss = 1.0107\n",
            "Batch 1100: Loss = 0.8982\n",
            "Batch 1200: Loss = 0.9433\n",
            "Batch 1300: Loss = 0.8776\n",
            "Batch 1400: Loss = 0.8267\n",
            "Batch 1500: Loss = 1.0876\n",
            "Batch 1600: Loss = 1.0120\n",
            "Batch 1700: Loss = 0.9280\n",
            "Batch 1800: Loss = 1.2087\n",
            "Epoch 14/50, Training Loss: 0.9608, Training Accuracy: 81.32%\n",
            "Batch 0: Loss = 1.1002\n",
            "Batch 100: Loss = 0.8829\n",
            "Batch 200: Loss = 0.8035\n",
            "Batch 300: Loss = 0.8427\n",
            "Batch 400: Loss = 1.0002\n",
            "Batch 500: Loss = 1.0157\n",
            "Batch 600: Loss = 0.8666\n",
            "Batch 700: Loss = 0.8264\n",
            "Batch 800: Loss = 0.8990\n",
            "Batch 900: Loss = 0.9832\n",
            "Batch 1000: Loss = 0.8918\n",
            "Batch 1100: Loss = 0.8051\n",
            "Batch 1200: Loss = 0.7745\n",
            "Batch 1300: Loss = 0.7919\n",
            "Batch 1400: Loss = 0.8739\n",
            "Batch 1500: Loss = 0.8631\n",
            "Batch 1600: Loss = 0.8736\n",
            "Batch 1700: Loss = 0.6424\n",
            "Batch 1800: Loss = 0.9878\n",
            "Epoch 15/50, Training Loss: 0.8758, Training Accuracy: 82.17%\n",
            "Batch 0: Loss = 0.6485\n",
            "Batch 100: Loss = 1.0332\n",
            "Batch 200: Loss = 0.6501\n",
            "Batch 300: Loss = 0.7699\n",
            "Batch 400: Loss = 0.8316\n",
            "Batch 500: Loss = 0.8889\n",
            "Batch 600: Loss = 0.9961\n",
            "Batch 700: Loss = 0.9057\n",
            "Batch 800: Loss = 0.6638\n",
            "Batch 900: Loss = 0.8186\n",
            "Batch 1000: Loss = 0.8286\n",
            "Batch 1100: Loss = 0.8504\n",
            "Batch 1200: Loss = 0.6909\n",
            "Batch 1300: Loss = 0.7328\n",
            "Batch 1400: Loss = 0.7488\n",
            "Batch 1500: Loss = 0.6362\n",
            "Batch 1600: Loss = 0.7797\n",
            "Batch 1700: Loss = 0.8196\n",
            "Batch 1800: Loss = 0.7692\n",
            "Epoch 16/50, Training Loss: 0.8064, Training Accuracy: 82.95%\n",
            "Batch 0: Loss = 0.9603\n",
            "Batch 100: Loss = 0.8388\n",
            "Batch 200: Loss = 0.9122\n",
            "Batch 300: Loss = 0.6921\n",
            "Batch 400: Loss = 1.0042\n",
            "Batch 500: Loss = 0.8920\n",
            "Batch 600: Loss = 0.8558\n",
            "Batch 700: Loss = 0.8429\n",
            "Batch 800: Loss = 0.6297\n",
            "Batch 900: Loss = 0.8203\n",
            "Batch 1000: Loss = 0.6222\n",
            "Batch 1100: Loss = 0.8844\n",
            "Batch 1200: Loss = 0.8021\n",
            "Batch 1300: Loss = 0.5817\n",
            "Batch 1400: Loss = 0.5947\n",
            "Batch 1500: Loss = 0.7772\n",
            "Batch 1600: Loss = 0.6535\n",
            "Batch 1700: Loss = 0.7831\n",
            "Batch 1800: Loss = 0.7826\n",
            "Epoch 17/50, Training Loss: 0.7492, Training Accuracy: 83.60%\n",
            "Batch 0: Loss = 0.7891\n",
            "Batch 100: Loss = 0.6957\n",
            "Batch 200: Loss = 0.5785\n",
            "Batch 300: Loss = 0.5984\n",
            "Batch 400: Loss = 0.5729\n",
            "Batch 500: Loss = 0.5854\n",
            "Batch 600: Loss = 0.7673\n",
            "Batch 700: Loss = 0.7855\n",
            "Batch 800: Loss = 0.8088\n",
            "Batch 900: Loss = 0.8200\n",
            "Batch 1000: Loss = 0.7423\n",
            "Batch 1100: Loss = 0.9026\n",
            "Batch 1200: Loss = 0.7429\n",
            "Batch 1300: Loss = 0.5356\n",
            "Batch 1400: Loss = 0.6402\n",
            "Batch 1500: Loss = 0.6785\n",
            "Batch 1600: Loss = 0.8134\n",
            "Batch 1700: Loss = 0.8521\n",
            "Batch 1800: Loss = 0.7330\n",
            "Epoch 18/50, Training Loss: 0.7017, Training Accuracy: 84.17%\n",
            "Batch 0: Loss = 0.7488\n",
            "Batch 100: Loss = 0.6607\n",
            "Batch 200: Loss = 0.6702\n",
            "Batch 300: Loss = 0.6658\n",
            "Batch 400: Loss = 0.5822\n",
            "Batch 500: Loss = 0.5372\n",
            "Batch 600: Loss = 0.7105\n",
            "Batch 700: Loss = 0.6401\n",
            "Batch 800: Loss = 0.7021\n",
            "Batch 900: Loss = 0.9589\n",
            "Batch 1000: Loss = 0.7688\n",
            "Batch 1100: Loss = 0.5628\n",
            "Batch 1200: Loss = 0.6412\n",
            "Batch 1300: Loss = 0.6929\n",
            "Batch 1400: Loss = 0.7582\n",
            "Batch 1500: Loss = 0.8886\n",
            "Batch 1600: Loss = 0.6110\n",
            "Batch 1700: Loss = 0.7400\n",
            "Batch 1800: Loss = 0.5281\n",
            "Epoch 19/50, Training Loss: 0.6621, Training Accuracy: 84.62%\n",
            "Batch 0: Loss = 0.6962\n",
            "Batch 100: Loss = 0.5825\n",
            "Batch 200: Loss = 0.6376\n",
            "Batch 300: Loss = 0.7097\n",
            "Batch 400: Loss = 0.6608\n",
            "Batch 500: Loss = 0.3848\n",
            "Batch 600: Loss = 0.5041\n",
            "Batch 700: Loss = 0.8705\n",
            "Batch 800: Loss = 0.4825\n",
            "Batch 900: Loss = 0.5026\n",
            "Batch 1000: Loss = 0.5934\n",
            "Batch 1100: Loss = 0.6899\n",
            "Batch 1200: Loss = 0.7148\n",
            "Batch 1300: Loss = 0.7598\n",
            "Batch 1400: Loss = 0.5209\n",
            "Batch 1500: Loss = 0.6493\n",
            "Batch 1600: Loss = 0.4935\n",
            "Batch 1700: Loss = 0.7737\n",
            "Batch 1800: Loss = 0.5941\n",
            "Epoch 20/50, Training Loss: 0.6287, Training Accuracy: 85.10%\n",
            "Batch 0: Loss = 0.8243\n",
            "Batch 100: Loss = 0.6563\n",
            "Batch 200: Loss = 0.5255\n",
            "Batch 300: Loss = 0.4719\n",
            "Batch 400: Loss = 0.6381\n",
            "Batch 500: Loss = 0.4263\n",
            "Batch 600: Loss = 0.8769\n",
            "Batch 700: Loss = 0.5877\n",
            "Batch 800: Loss = 0.6008\n",
            "Batch 900: Loss = 0.5884\n",
            "Batch 1000: Loss = 0.5817\n",
            "Batch 1100: Loss = 0.7266\n",
            "Batch 1200: Loss = 0.5697\n",
            "Batch 1300: Loss = 0.5411\n",
            "Batch 1400: Loss = 0.8492\n",
            "Batch 1500: Loss = 0.4596\n",
            "Batch 1600: Loss = 0.4647\n",
            "Batch 1700: Loss = 0.5305\n",
            "Batch 1800: Loss = 0.5497\n",
            "Epoch 21/50, Training Loss: 0.6001, Training Accuracy: 85.52%\n",
            "Batch 0: Loss = 0.7547\n",
            "Batch 100: Loss = 0.5999\n",
            "Batch 200: Loss = 0.4216\n",
            "Batch 300: Loss = 0.5216\n",
            "Batch 400: Loss = 0.5272\n",
            "Batch 500: Loss = 0.4936\n",
            "Batch 600: Loss = 0.4415\n",
            "Batch 700: Loss = 0.6124\n",
            "Batch 800: Loss = 0.7285\n",
            "Batch 900: Loss = 0.4408\n",
            "Batch 1000: Loss = 0.6736\n",
            "Batch 1100: Loss = 0.4505\n",
            "Batch 1200: Loss = 0.5221\n",
            "Batch 1300: Loss = 0.5277\n",
            "Batch 1400: Loss = 0.6164\n",
            "Batch 1500: Loss = 0.5740\n",
            "Batch 1600: Loss = 0.5037\n",
            "Batch 1700: Loss = 0.5639\n",
            "Batch 1800: Loss = 0.5734\n",
            "Epoch 22/50, Training Loss: 0.5754, Training Accuracy: 85.88%\n",
            "Batch 0: Loss = 0.5118\n",
            "Batch 100: Loss = 0.6653\n",
            "Batch 200: Loss = 0.4748\n",
            "Batch 300: Loss = 0.8648\n",
            "Batch 400: Loss = 0.4263\n",
            "Batch 500: Loss = 0.6437\n",
            "Batch 600: Loss = 0.5535\n",
            "Batch 700: Loss = 0.5045\n",
            "Batch 800: Loss = 0.9712\n",
            "Batch 900: Loss = 0.4904\n",
            "Batch 1000: Loss = 0.3859\n",
            "Batch 1100: Loss = 0.5682\n",
            "Batch 1200: Loss = 0.6743\n",
            "Batch 1300: Loss = 0.5077\n",
            "Batch 1400: Loss = 0.5723\n",
            "Batch 1500: Loss = 0.3949\n",
            "Batch 1600: Loss = 0.7425\n",
            "Batch 1700: Loss = 0.6557\n",
            "Batch 1800: Loss = 0.3546\n",
            "Epoch 23/50, Training Loss: 0.5538, Training Accuracy: 86.21%\n",
            "Batch 0: Loss = 0.4983\n",
            "Batch 100: Loss = 0.4812\n",
            "Batch 200: Loss = 0.6028\n",
            "Batch 300: Loss = 0.5488\n",
            "Batch 400: Loss = 0.4817\n",
            "Batch 500: Loss = 0.5100\n",
            "Batch 600: Loss = 0.6482\n",
            "Batch 700: Loss = 0.5853\n",
            "Batch 800: Loss = 0.5214\n",
            "Batch 900: Loss = 0.5711\n",
            "Batch 1000: Loss = 0.7556\n",
            "Batch 1100: Loss = 0.3346\n",
            "Batch 1200: Loss = 0.5037\n",
            "Batch 1300: Loss = 0.4715\n",
            "Batch 1400: Loss = 0.3591\n",
            "Batch 1500: Loss = 0.4383\n",
            "Batch 1600: Loss = 0.4202\n",
            "Batch 1700: Loss = 0.6401\n",
            "Batch 1800: Loss = 0.5890\n",
            "Epoch 24/50, Training Loss: 0.5348, Training Accuracy: 86.53%\n",
            "Batch 0: Loss = 0.5473\n",
            "Batch 100: Loss = 0.5775\n",
            "Batch 200: Loss = 0.5127\n",
            "Batch 300: Loss = 0.5305\n",
            "Batch 400: Loss = 0.5555\n",
            "Batch 500: Loss = 0.4853\n",
            "Batch 600: Loss = 0.4580\n",
            "Batch 700: Loss = 0.6323\n",
            "Batch 800: Loss = 0.6302\n",
            "Batch 900: Loss = 0.4283\n",
            "Batch 1000: Loss = 0.4979\n",
            "Batch 1100: Loss = 0.4271\n",
            "Batch 1200: Loss = 0.6425\n",
            "Batch 1300: Loss = 0.5805\n",
            "Batch 1400: Loss = 0.4775\n",
            "Batch 1500: Loss = 0.4043\n",
            "Batch 1600: Loss = 0.5805\n",
            "Batch 1700: Loss = 0.6297\n",
            "Batch 1800: Loss = 0.3988\n",
            "Epoch 25/50, Training Loss: 0.5179, Training Accuracy: 86.76%\n",
            "Batch 0: Loss = 0.3916\n",
            "Batch 100: Loss = 0.3472\n",
            "Batch 200: Loss = 0.5489\n",
            "Batch 300: Loss = 0.3356\n",
            "Batch 400: Loss = 0.5055\n",
            "Batch 500: Loss = 0.5812\n",
            "Batch 600: Loss = 0.6342\n",
            "Batch 700: Loss = 0.4524\n",
            "Batch 800: Loss = 0.5038\n",
            "Batch 900: Loss = 0.5638\n",
            "Batch 1000: Loss = 0.9257\n",
            "Batch 1100: Loss = 0.4399\n",
            "Batch 1200: Loss = 0.7846\n",
            "Batch 1300: Loss = 0.4287\n",
            "Batch 1400: Loss = 0.4812\n",
            "Batch 1500: Loss = 0.3787\n",
            "Batch 1600: Loss = 0.9740\n",
            "Batch 1700: Loss = 0.5233\n",
            "Batch 1800: Loss = 0.3050\n",
            "Epoch 26/50, Training Loss: 0.5029, Training Accuracy: 87.06%\n",
            "Batch 0: Loss = 0.8188\n",
            "Batch 100: Loss = 0.6991\n",
            "Batch 200: Loss = 0.3117\n",
            "Batch 300: Loss = 0.5392\n",
            "Batch 400: Loss = 0.5806\n",
            "Batch 500: Loss = 0.7298\n",
            "Batch 600: Loss = 0.2929\n",
            "Batch 700: Loss = 0.6997\n",
            "Batch 800: Loss = 0.2268\n",
            "Batch 900: Loss = 0.4501\n",
            "Batch 1000: Loss = 0.5930\n",
            "Batch 1100: Loss = 0.5449\n",
            "Batch 1200: Loss = 0.4683\n",
            "Batch 1300: Loss = 0.4769\n",
            "Batch 1400: Loss = 0.3602\n",
            "Batch 1500: Loss = 0.4338\n",
            "Batch 1600: Loss = 0.3955\n",
            "Batch 1700: Loss = 0.3114\n",
            "Batch 1800: Loss = 0.6266\n",
            "Epoch 27/50, Training Loss: 0.4893, Training Accuracy: 87.27%\n",
            "Batch 0: Loss = 0.4824\n",
            "Batch 100: Loss = 0.4091\n",
            "Batch 200: Loss = 0.4205\n",
            "Batch 300: Loss = 0.3432\n",
            "Batch 400: Loss = 0.3650\n",
            "Batch 500: Loss = 0.2826\n",
            "Batch 600: Loss = 0.4153\n",
            "Batch 700: Loss = 0.5687\n",
            "Batch 800: Loss = 0.4267\n",
            "Batch 900: Loss = 0.2540\n",
            "Batch 1000: Loss = 0.6809\n",
            "Batch 1100: Loss = 0.5962\n",
            "Batch 1200: Loss = 0.4201\n",
            "Batch 1300: Loss = 0.5640\n",
            "Batch 1400: Loss = 0.3770\n",
            "Batch 1500: Loss = 0.3439\n",
            "Batch 1600: Loss = 0.2549\n",
            "Batch 1700: Loss = 0.6159\n",
            "Batch 1800: Loss = 0.6207\n",
            "Epoch 28/50, Training Loss: 0.4770, Training Accuracy: 87.49%\n",
            "Batch 0: Loss = 0.3403\n",
            "Batch 100: Loss = 0.3785\n",
            "Batch 200: Loss = 0.4664\n",
            "Batch 300: Loss = 0.4826\n",
            "Batch 400: Loss = 0.2312\n",
            "Batch 500: Loss = 0.3029\n",
            "Batch 600: Loss = 0.5356\n",
            "Batch 700: Loss = 0.3750\n",
            "Batch 800: Loss = 0.5193\n",
            "Batch 900: Loss = 0.6366\n",
            "Batch 1000: Loss = 0.3556\n",
            "Batch 1100: Loss = 0.6046\n",
            "Batch 1200: Loss = 0.3282\n",
            "Batch 1300: Loss = 0.3988\n",
            "Batch 1400: Loss = 0.4432\n",
            "Batch 1500: Loss = 0.3145\n",
            "Batch 1600: Loss = 0.4286\n",
            "Batch 1700: Loss = 0.6248\n",
            "Batch 1800: Loss = 0.5367\n",
            "Epoch 29/50, Training Loss: 0.4659, Training Accuracy: 87.71%\n",
            "Batch 0: Loss = 0.4799\n",
            "Batch 100: Loss = 0.3847\n",
            "Batch 200: Loss = 0.3342\n",
            "Batch 300: Loss = 0.5353\n",
            "Batch 400: Loss = 0.4041\n",
            "Batch 500: Loss = 0.5606\n",
            "Batch 600: Loss = 0.4849\n",
            "Batch 700: Loss = 0.4815\n",
            "Batch 800: Loss = 0.4354\n",
            "Batch 900: Loss = 0.4421\n",
            "Batch 1000: Loss = 0.3043\n",
            "Batch 1100: Loss = 0.4919\n",
            "Batch 1200: Loss = 0.3613\n",
            "Batch 1300: Loss = 0.3783\n",
            "Batch 1400: Loss = 0.4434\n",
            "Batch 1500: Loss = 0.4508\n",
            "Batch 1600: Loss = 0.3168\n",
            "Batch 1700: Loss = 0.5610\n",
            "Batch 1800: Loss = 0.5399\n",
            "Epoch 30/50, Training Loss: 0.4558, Training Accuracy: 87.90%\n",
            "Batch 0: Loss = 0.5881\n",
            "Batch 100: Loss = 0.3059\n",
            "Batch 200: Loss = 0.3377\n",
            "Batch 300: Loss = 0.5336\n",
            "Batch 400: Loss = 0.5014\n",
            "Batch 500: Loss = 0.4890\n",
            "Batch 600: Loss = 0.3133\n",
            "Batch 700: Loss = 0.7430\n",
            "Batch 800: Loss = 0.2588\n",
            "Batch 900: Loss = 0.1893\n",
            "Batch 1000: Loss = 0.4998\n",
            "Batch 1100: Loss = 0.3503\n",
            "Batch 1200: Loss = 0.4136\n",
            "Batch 1300: Loss = 0.4916\n",
            "Batch 1400: Loss = 0.4244\n",
            "Batch 1500: Loss = 0.6298\n",
            "Batch 1600: Loss = 0.5129\n",
            "Batch 1700: Loss = 0.2066\n",
            "Batch 1800: Loss = 0.5229\n",
            "Epoch 31/50, Training Loss: 0.4465, Training Accuracy: 88.08%\n",
            "Batch 0: Loss = 0.5155\n",
            "Batch 100: Loss = 0.2240\n",
            "Batch 200: Loss = 0.7804\n",
            "Batch 300: Loss = 0.7555\n",
            "Batch 400: Loss = 0.6023\n",
            "Batch 500: Loss = 0.2738\n",
            "Batch 600: Loss = 0.5998\n",
            "Batch 700: Loss = 0.4695\n",
            "Batch 800: Loss = 0.5731\n",
            "Batch 900: Loss = 0.6066\n",
            "Batch 1000: Loss = 0.4583\n",
            "Batch 1100: Loss = 0.4971\n",
            "Batch 1200: Loss = 0.3141\n",
            "Batch 1300: Loss = 0.3759\n",
            "Batch 1400: Loss = 0.5120\n",
            "Batch 1500: Loss = 0.3526\n",
            "Batch 1600: Loss = 0.2958\n",
            "Batch 1700: Loss = 0.2608\n",
            "Batch 1800: Loss = 0.5552\n",
            "Epoch 32/50, Training Loss: 0.4380, Training Accuracy: 88.27%\n",
            "Batch 0: Loss = 0.2922\n",
            "Batch 100: Loss = 0.6597\n",
            "Batch 200: Loss = 0.3926\n",
            "Batch 300: Loss = 0.3561\n",
            "Batch 400: Loss = 0.3400\n",
            "Batch 500: Loss = 0.4214\n",
            "Batch 600: Loss = 0.4110\n",
            "Batch 700: Loss = 0.3843\n",
            "Batch 800: Loss = 0.2393\n",
            "Batch 900: Loss = 0.5236\n",
            "Batch 1000: Loss = 0.5159\n",
            "Batch 1100: Loss = 0.2770\n",
            "Batch 1200: Loss = 0.3417\n",
            "Batch 1300: Loss = 0.2099\n",
            "Batch 1400: Loss = 0.4520\n",
            "Batch 1500: Loss = 0.4101\n",
            "Batch 1600: Loss = 0.3978\n",
            "Batch 1700: Loss = 0.5944\n",
            "Batch 1800: Loss = 0.3850\n",
            "Epoch 33/50, Training Loss: 0.4301, Training Accuracy: 88.43%\n",
            "Batch 0: Loss = 0.2877\n",
            "Batch 100: Loss = 0.4384\n",
            "Batch 200: Loss = 0.4436\n",
            "Batch 300: Loss = 0.2855\n",
            "Batch 400: Loss = 0.2609\n",
            "Batch 500: Loss = 0.3015\n",
            "Batch 600: Loss = 0.5681\n",
            "Batch 700: Loss = 0.3864\n",
            "Batch 800: Loss = 0.4852\n",
            "Batch 900: Loss = 0.7682\n",
            "Batch 1000: Loss = 0.3854\n",
            "Batch 1100: Loss = 0.3717\n",
            "Batch 1200: Loss = 0.5037\n",
            "Batch 1300: Loss = 0.5467\n",
            "Batch 1400: Loss = 0.5491\n",
            "Batch 1500: Loss = 0.4032\n",
            "Batch 1600: Loss = 0.3208\n",
            "Batch 1700: Loss = 0.3868\n",
            "Batch 1800: Loss = 0.5909\n",
            "Epoch 34/50, Training Loss: 0.4229, Training Accuracy: 88.56%\n",
            "Batch 0: Loss = 0.7317\n",
            "Batch 100: Loss = 0.3589\n",
            "Batch 200: Loss = 0.5375\n",
            "Batch 300: Loss = 0.3584\n",
            "Batch 400: Loss = 0.2746\n",
            "Batch 500: Loss = 0.4503\n",
            "Batch 600: Loss = 0.4451\n",
            "Batch 700: Loss = 0.3298\n",
            "Batch 800: Loss = 0.2442\n",
            "Batch 900: Loss = 0.3923\n",
            "Batch 1000: Loss = 0.3458\n",
            "Batch 1100: Loss = 0.3250\n",
            "Batch 1200: Loss = 0.5692\n",
            "Batch 1300: Loss = 0.5295\n",
            "Batch 1400: Loss = 0.2793\n",
            "Batch 1500: Loss = 0.2480\n",
            "Batch 1600: Loss = 0.3687\n",
            "Batch 1700: Loss = 0.2643\n",
            "Batch 1800: Loss = 0.6068\n",
            "Epoch 35/50, Training Loss: 0.4161, Training Accuracy: 88.68%\n",
            "Batch 0: Loss = 0.5689\n",
            "Batch 100: Loss = 0.3770\n",
            "Batch 200: Loss = 0.3723\n",
            "Batch 300: Loss = 0.3300\n",
            "Batch 400: Loss = 0.6994\n",
            "Batch 500: Loss = 0.4430\n",
            "Batch 600: Loss = 0.4261\n",
            "Batch 700: Loss = 0.5366\n",
            "Batch 800: Loss = 0.4384\n",
            "Batch 900: Loss = 0.3964\n",
            "Batch 1000: Loss = 0.5779\n",
            "Batch 1100: Loss = 0.3169\n",
            "Batch 1200: Loss = 0.6277\n",
            "Batch 1300: Loss = 0.3458\n",
            "Batch 1400: Loss = 0.2275\n",
            "Batch 1500: Loss = 0.5076\n",
            "Batch 1600: Loss = 0.3473\n",
            "Batch 1700: Loss = 0.2730\n",
            "Batch 1800: Loss = 0.2858\n",
            "Epoch 36/50, Training Loss: 0.4099, Training Accuracy: 88.82%\n",
            "Batch 0: Loss = 0.4100\n",
            "Batch 100: Loss = 0.3340\n",
            "Batch 200: Loss = 0.3441\n",
            "Batch 300: Loss = 0.2333\n",
            "Batch 400: Loss = 0.1315\n",
            "Batch 500: Loss = 0.4355\n",
            "Batch 600: Loss = 0.2745\n",
            "Batch 700: Loss = 0.2124\n",
            "Batch 800: Loss = 0.4647\n",
            "Batch 900: Loss = 0.3865\n",
            "Batch 1000: Loss = 0.3071\n",
            "Batch 1100: Loss = 0.3735\n",
            "Batch 1200: Loss = 0.4125\n",
            "Batch 1300: Loss = 0.3323\n",
            "Batch 1400: Loss = 0.2121\n",
            "Batch 1500: Loss = 0.2383\n",
            "Batch 1600: Loss = 0.5546\n",
            "Batch 1700: Loss = 0.4870\n",
            "Batch 1800: Loss = 0.4053\n",
            "Epoch 37/50, Training Loss: 0.4040, Training Accuracy: 88.96%\n",
            "Batch 0: Loss = 0.4915\n",
            "Batch 100: Loss = 0.6629\n",
            "Batch 200: Loss = 0.4475\n",
            "Batch 300: Loss = 0.3373\n",
            "Batch 400: Loss = 0.3989\n",
            "Batch 500: Loss = 0.2423\n",
            "Batch 600: Loss = 0.5199\n",
            "Batch 700: Loss = 0.4834\n",
            "Batch 800: Loss = 0.6272\n",
            "Batch 900: Loss = 0.5119\n",
            "Batch 1000: Loss = 0.4471\n",
            "Batch 1100: Loss = 0.3762\n",
            "Batch 1200: Loss = 0.6130\n",
            "Batch 1300: Loss = 0.2917\n",
            "Batch 1400: Loss = 0.6429\n",
            "Batch 1500: Loss = 0.3810\n",
            "Batch 1600: Loss = 0.3942\n",
            "Batch 1700: Loss = 0.4413\n",
            "Batch 1800: Loss = 0.1789\n",
            "Epoch 38/50, Training Loss: 0.3986, Training Accuracy: 89.05%\n",
            "Batch 0: Loss = 0.3784\n",
            "Batch 100: Loss = 0.4050\n",
            "Batch 200: Loss = 0.6073\n",
            "Batch 300: Loss = 0.3786\n",
            "Batch 400: Loss = 0.2866\n",
            "Batch 500: Loss = 0.3382\n",
            "Batch 600: Loss = 0.2609\n",
            "Batch 700: Loss = 0.3251\n",
            "Batch 800: Loss = 0.5980\n",
            "Batch 900: Loss = 0.4256\n",
            "Batch 1000: Loss = 0.5119\n",
            "Batch 1100: Loss = 0.2220\n",
            "Batch 1200: Loss = 0.2782\n",
            "Batch 1300: Loss = 0.4018\n",
            "Batch 1400: Loss = 0.2917\n",
            "Batch 1500: Loss = 0.2929\n",
            "Batch 1600: Loss = 0.3730\n",
            "Batch 1700: Loss = 0.2161\n",
            "Batch 1800: Loss = 0.4022\n",
            "Epoch 39/50, Training Loss: 0.3934, Training Accuracy: 89.16%\n",
            "Batch 0: Loss = 0.2147\n",
            "Batch 100: Loss = 0.4715\n",
            "Batch 200: Loss = 0.5558\n",
            "Batch 300: Loss = 0.6173\n",
            "Batch 400: Loss = 0.1863\n",
            "Batch 500: Loss = 0.1883\n",
            "Batch 600: Loss = 0.3631\n",
            "Batch 700: Loss = 0.4406\n",
            "Batch 800: Loss = 0.4041\n",
            "Batch 900: Loss = 0.2537\n",
            "Batch 1000: Loss = 0.3944\n",
            "Batch 1100: Loss = 0.1516\n",
            "Batch 1200: Loss = 0.2957\n",
            "Batch 1300: Loss = 0.3445\n",
            "Batch 1400: Loss = 0.2490\n",
            "Batch 1500: Loss = 0.3539\n",
            "Batch 1600: Loss = 0.5109\n",
            "Batch 1700: Loss = 0.3376\n",
            "Batch 1800: Loss = 0.2248\n",
            "Epoch 40/50, Training Loss: 0.3886, Training Accuracy: 89.29%\n",
            "Batch 0: Loss = 0.2725\n",
            "Batch 100: Loss = 0.7346\n",
            "Batch 200: Loss = 0.3908\n",
            "Batch 300: Loss = 0.4926\n",
            "Batch 400: Loss = 0.4062\n",
            "Batch 500: Loss = 0.5701\n",
            "Batch 600: Loss = 0.5185\n",
            "Batch 700: Loss = 0.4956\n",
            "Batch 800: Loss = 0.3165\n",
            "Batch 900: Loss = 0.1724\n",
            "Batch 1000: Loss = 0.2054\n",
            "Batch 1100: Loss = 0.3035\n",
            "Batch 1200: Loss = 0.7329\n",
            "Batch 1300: Loss = 0.4970\n",
            "Batch 1400: Loss = 0.3187\n",
            "Batch 1500: Loss = 0.4406\n",
            "Batch 1600: Loss = 0.2916\n",
            "Batch 1700: Loss = 0.4415\n",
            "Batch 1800: Loss = 0.3917\n",
            "Epoch 41/50, Training Loss: 0.3841, Training Accuracy: 89.39%\n",
            "Batch 0: Loss = 0.3921\n",
            "Batch 100: Loss = 0.2291\n",
            "Batch 200: Loss = 0.5368\n",
            "Batch 300: Loss = 0.2961\n",
            "Batch 400: Loss = 0.4644\n",
            "Batch 500: Loss = 0.2768\n",
            "Batch 600: Loss = 0.4412\n",
            "Batch 700: Loss = 0.4236\n",
            "Batch 800: Loss = 0.2186\n",
            "Batch 900: Loss = 0.2203\n",
            "Batch 1000: Loss = 0.2669\n",
            "Batch 1100: Loss = 0.6697\n",
            "Batch 1200: Loss = 0.5233\n",
            "Batch 1300: Loss = 0.4596\n",
            "Batch 1400: Loss = 0.2605\n",
            "Batch 1500: Loss = 0.3241\n",
            "Batch 1600: Loss = 0.2737\n",
            "Batch 1700: Loss = 0.2949\n",
            "Batch 1800: Loss = 0.2491\n",
            "Epoch 42/50, Training Loss: 0.3798, Training Accuracy: 89.44%\n",
            "Batch 0: Loss = 0.4765\n",
            "Batch 100: Loss = 0.3885\n",
            "Batch 200: Loss = 0.4804\n",
            "Batch 300: Loss = 0.5750\n",
            "Batch 400: Loss = 0.4163\n",
            "Batch 500: Loss = 0.4614\n",
            "Batch 600: Loss = 0.3283\n",
            "Batch 700: Loss = 0.4624\n",
            "Batch 800: Loss = 0.2388\n",
            "Batch 900: Loss = 0.3570\n",
            "Batch 1000: Loss = 0.2925\n",
            "Batch 1100: Loss = 0.5110\n",
            "Batch 1200: Loss = 0.4019\n",
            "Batch 1300: Loss = 0.4548\n",
            "Batch 1400: Loss = 0.2974\n",
            "Batch 1500: Loss = 0.2693\n",
            "Batch 1600: Loss = 0.1571\n",
            "Batch 1700: Loss = 0.4513\n",
            "Batch 1800: Loss = 0.2220\n",
            "Epoch 43/50, Training Loss: 0.3758, Training Accuracy: 89.53%\n",
            "Batch 0: Loss = 0.4080\n",
            "Batch 100: Loss = 0.4669\n",
            "Batch 200: Loss = 0.2738\n",
            "Batch 300: Loss = 0.2492\n",
            "Batch 400: Loss = 0.5376\n",
            "Batch 500: Loss = 0.5935\n",
            "Batch 600: Loss = 0.2942\n",
            "Batch 700: Loss = 0.3726\n",
            "Batch 800: Loss = 0.1430\n",
            "Batch 900: Loss = 0.3303\n",
            "Batch 1000: Loss = 0.2313\n",
            "Batch 1100: Loss = 0.6536\n",
            "Batch 1200: Loss = 0.6415\n",
            "Batch 1300: Loss = 0.5356\n",
            "Batch 1400: Loss = 0.3980\n",
            "Batch 1500: Loss = 0.1944\n",
            "Batch 1600: Loss = 0.2317\n",
            "Batch 1700: Loss = 0.6221\n",
            "Batch 1800: Loss = 0.1623\n",
            "Epoch 44/50, Training Loss: 0.3719, Training Accuracy: 89.60%\n",
            "Batch 0: Loss = 0.2438\n",
            "Batch 100: Loss = 0.3301\n",
            "Batch 200: Loss = 0.2439\n",
            "Batch 300: Loss = 0.3350\n",
            "Batch 400: Loss = 0.2679\n",
            "Batch 500: Loss = 0.3195\n",
            "Batch 600: Loss = 0.3112\n",
            "Batch 700: Loss = 0.1525\n",
            "Batch 800: Loss = 0.2866\n",
            "Batch 900: Loss = 0.4361\n",
            "Batch 1000: Loss = 0.2742\n",
            "Batch 1100: Loss = 0.2485\n",
            "Batch 1200: Loss = 0.3293\n",
            "Batch 1300: Loss = 0.5128\n",
            "Batch 1400: Loss = 0.4075\n",
            "Batch 1500: Loss = 0.4852\n",
            "Batch 1600: Loss = 0.2571\n",
            "Batch 1700: Loss = 0.2389\n",
            "Batch 1800: Loss = 0.6778\n",
            "Epoch 45/50, Training Loss: 0.3683, Training Accuracy: 89.70%\n",
            "Batch 0: Loss = 0.4478\n",
            "Batch 100: Loss = 0.2392\n",
            "Batch 200: Loss = 0.3872\n",
            "Batch 300: Loss = 0.3785\n",
            "Batch 400: Loss = 0.3093\n",
            "Batch 500: Loss = 0.4506\n",
            "Batch 600: Loss = 0.3198\n",
            "Batch 700: Loss = 0.2298\n",
            "Batch 800: Loss = 0.2108\n",
            "Batch 900: Loss = 0.3033\n",
            "Batch 1000: Loss = 0.2992\n",
            "Batch 1100: Loss = 0.3628\n",
            "Batch 1200: Loss = 0.4926\n",
            "Batch 1300: Loss = 0.2455\n",
            "Batch 1400: Loss = 0.4919\n",
            "Batch 1500: Loss = 0.1044\n",
            "Batch 1600: Loss = 0.4648\n",
            "Batch 1700: Loss = 0.4305\n",
            "Batch 1800: Loss = 0.2821\n",
            "Epoch 46/50, Training Loss: 0.3648, Training Accuracy: 89.74%\n",
            "Batch 0: Loss = 0.2348\n",
            "Batch 100: Loss = 0.2378\n",
            "Batch 200: Loss = 0.3251\n",
            "Batch 300: Loss = 0.2671\n",
            "Batch 400: Loss = 0.5071\n",
            "Batch 500: Loss = 0.2883\n",
            "Batch 600: Loss = 0.2410\n",
            "Batch 700: Loss = 0.4615\n",
            "Batch 800: Loss = 0.3614\n",
            "Batch 900: Loss = 0.3581\n",
            "Batch 1000: Loss = 0.3353\n",
            "Batch 1100: Loss = 0.6620\n",
            "Batch 1200: Loss = 0.2418\n",
            "Batch 1300: Loss = 0.3458\n",
            "Batch 1400: Loss = 0.1906\n",
            "Batch 1500: Loss = 0.1951\n",
            "Batch 1600: Loss = 0.2016\n",
            "Batch 1700: Loss = 0.2423\n",
            "Batch 1800: Loss = 0.3802\n",
            "Epoch 47/50, Training Loss: 0.3615, Training Accuracy: 89.86%\n",
            "Batch 0: Loss = 0.3177\n",
            "Batch 100: Loss = 0.3146\n",
            "Batch 200: Loss = 0.3123\n",
            "Batch 300: Loss = 0.4821\n",
            "Batch 400: Loss = 0.3263\n",
            "Batch 500: Loss = 0.4982\n",
            "Batch 600: Loss = 0.4804\n",
            "Batch 700: Loss = 0.3589\n",
            "Batch 800: Loss = 0.2787\n",
            "Batch 900: Loss = 0.5248\n",
            "Batch 1000: Loss = 0.1581\n",
            "Batch 1100: Loss = 0.3844\n",
            "Batch 1200: Loss = 0.3596\n",
            "Batch 1300: Loss = 0.2913\n",
            "Batch 1400: Loss = 0.5895\n",
            "Batch 1500: Loss = 0.3467\n",
            "Batch 1600: Loss = 0.2098\n",
            "Batch 1700: Loss = 0.5898\n",
            "Batch 1800: Loss = 0.7060\n",
            "Epoch 48/50, Training Loss: 0.3583, Training Accuracy: 89.91%\n",
            "Batch 0: Loss = 0.5226\n",
            "Batch 100: Loss = 0.2280\n",
            "Batch 200: Loss = 0.3622\n",
            "Batch 300: Loss = 0.2852\n",
            "Batch 400: Loss = 0.2969\n",
            "Batch 500: Loss = 0.6910\n",
            "Batch 600: Loss = 0.3695\n",
            "Batch 700: Loss = 0.4369\n",
            "Batch 800: Loss = 0.3454\n",
            "Batch 900: Loss = 0.3932\n",
            "Batch 1000: Loss = 0.2709\n",
            "Batch 1100: Loss = 0.1664\n",
            "Batch 1200: Loss = 0.2677\n",
            "Batch 1300: Loss = 0.0903\n",
            "Batch 1400: Loss = 0.2924\n",
            "Batch 1500: Loss = 0.1382\n",
            "Batch 1600: Loss = 0.4421\n",
            "Batch 1700: Loss = 0.4354\n",
            "Batch 1800: Loss = 0.4587\n",
            "Epoch 49/50, Training Loss: 0.3553, Training Accuracy: 89.98%\n",
            "Batch 0: Loss = 0.5183\n",
            "Batch 100: Loss = 0.2952\n",
            "Batch 200: Loss = 0.2690\n",
            "Batch 300: Loss = 0.2800\n",
            "Batch 400: Loss = 0.3699\n",
            "Batch 500: Loss = 0.2288\n",
            "Batch 600: Loss = 0.2716\n",
            "Batch 700: Loss = 0.5803\n",
            "Batch 800: Loss = 1.0192\n",
            "Batch 900: Loss = 0.2623\n",
            "Batch 1000: Loss = 0.5073\n",
            "Batch 1100: Loss = 0.2670\n",
            "Batch 1200: Loss = 0.4428\n",
            "Batch 1300: Loss = 0.6758\n",
            "Batch 1400: Loss = 0.3832\n",
            "Batch 1500: Loss = 0.2427\n",
            "Batch 1600: Loss = 0.6432\n",
            "Batch 1700: Loss = 0.5593\n",
            "Batch 1800: Loss = 0.4065\n",
            "Epoch 50/50, Training Loss: 0.3524, Training Accuracy: 90.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Flatten images\n",
        "            images = images.view(images.shape[0], -1)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            predictions = torch.argmax(outputs, dim=1)  # Get the predicted class\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_corrects += correct\n",
        "            total_samples += labels.shape[0]\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    acc = (total_corrects / total_samples) * 100.0\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4PhtIX_1fjH",
        "outputId": "8e6e43e7-ea78-4ed9-b5ad-5e2704fd0223"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.3354, Test Accuracy: 90.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP with Skip Connections and Layer Normalization for 10 Class Classification"
      ],
      "metadata": {
        "id": "4r94gSTG6R5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the MLP model with skip connections between d1 and d2\n",
        "class MLP_Skip_LN(nn.Module):\n",
        "    def __init__(self, input_dim, d1, num_classes):\n",
        "        super(MLP_Skip_LN, self).__init__()\n",
        "\n",
        "        # Keep d1 = d2 for skip connection compatibility\n",
        "        self.hidden1 = nn.Linear(input_dim, d1)\n",
        "        self.hidden2 = nn.Linear(d1, d1)  # d1 = d2 (same size)\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(d1)  # LayerNorm AFTER activation\n",
        "        self.ln2 = nn.LayerNorm(d1)  # LayerNorm AFTER activation\n",
        "\n",
        "        self.output = nn.Linear(d1, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = torch.relu(self.hidden1(x))  # First hidden layer activation\n",
        "        x1 = self.ln1(x1)  # Apply LayerNorm AFTER activation\n",
        "\n",
        "        x2 = torch.relu(self.hidden2(x1) + x1)  # Apply activation AFTER skip connection\n",
        "        x2 = self.ln2(x2)  # Apply LayerNorm AFTER activation\n",
        "\n",
        "        x = self.output(x2)  # Final classification layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "2huZYQtHFM-a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "input_dim = 28 * 28  # MNIST images are 28x28\n",
        "d1 = 128  # Keep d1 = d2\n",
        "num_classes = 10  # 10-class classification\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 50"
      ],
      "metadata": {
        "id": "tMY9daQiFQB-"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = MLP_Skip_LN(input_dim, d1, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        images = images.view(images.shape[0], -1)  # Flatten images\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = torch.argmax(outputs, dim=1)\n",
        "        correct = (predictions == labels).sum().item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_corrects += correct\n",
        "        total_samples += labels.shape[0]\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    acc = (total_corrects / total_samples) * 100.0\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIMtipmV7dEP",
        "outputId": "7dd2009a-5dce-4c58-eb1f-9c8d6127877e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: Loss = 2.5009\n",
            "Batch 100: Loss = 2.2595\n",
            "Batch 200: Loss = 2.2310\n",
            "Batch 300: Loss = 1.9587\n",
            "Batch 400: Loss = 1.8364\n",
            "Batch 500: Loss = 1.8384\n",
            "Batch 600: Loss = 1.6636\n",
            "Batch 700: Loss = 1.5116\n",
            "Batch 800: Loss = 1.5167\n",
            "Batch 900: Loss = 1.4683\n",
            "Batch 1000: Loss = 1.7266\n",
            "Batch 1100: Loss = 1.2691\n",
            "Batch 1200: Loss = 1.3220\n",
            "Batch 1300: Loss = 1.2085\n",
            "Batch 1400: Loss = 1.3784\n",
            "Batch 1500: Loss = 1.1088\n",
            "Batch 1600: Loss = 1.3543\n",
            "Batch 1700: Loss = 1.0978\n",
            "Batch 1800: Loss = 1.0065\n",
            "Epoch 1/50, Training Loss: 1.5755, Training Accuracy: 55.44%\n",
            "Batch 0: Loss = 1.1135\n",
            "Batch 100: Loss = 0.9370\n",
            "Batch 200: Loss = 1.0227\n",
            "Batch 300: Loss = 1.1352\n",
            "Batch 400: Loss = 0.9657\n",
            "Batch 500: Loss = 1.1235\n",
            "Batch 600: Loss = 1.0308\n",
            "Batch 700: Loss = 0.9307\n",
            "Batch 800: Loss = 0.9476\n",
            "Batch 900: Loss = 0.9333\n",
            "Batch 1000: Loss = 0.8630\n",
            "Batch 1100: Loss = 0.9775\n",
            "Batch 1200: Loss = 0.8438\n",
            "Batch 1300: Loss = 0.7283\n",
            "Batch 1400: Loss = 0.7531\n",
            "Batch 1500: Loss = 0.7281\n",
            "Batch 1600: Loss = 0.8864\n",
            "Batch 1700: Loss = 0.8079\n",
            "Batch 1800: Loss = 0.8719\n",
            "Epoch 2/50, Training Loss: 0.9449, Training Accuracy: 80.00%\n",
            "Batch 0: Loss = 0.7387\n",
            "Batch 100: Loss = 0.8729\n",
            "Batch 200: Loss = 0.7448\n",
            "Batch 300: Loss = 0.7585\n",
            "Batch 400: Loss = 0.8267\n",
            "Batch 500: Loss = 0.6501\n",
            "Batch 600: Loss = 0.7342\n",
            "Batch 700: Loss = 0.8404\n",
            "Batch 800: Loss = 0.7989\n",
            "Batch 900: Loss = 0.6639\n",
            "Batch 1000: Loss = 0.7835\n",
            "Batch 1100: Loss = 0.6144\n",
            "Batch 1200: Loss = 0.5607\n",
            "Batch 1300: Loss = 0.6749\n",
            "Batch 1400: Loss = 0.7930\n",
            "Batch 1500: Loss = 0.5248\n",
            "Batch 1600: Loss = 0.5490\n",
            "Batch 1700: Loss = 0.7030\n",
            "Batch 1800: Loss = 0.6381\n",
            "Epoch 3/50, Training Loss: 0.7349, Training Accuracy: 84.52%\n",
            "Batch 0: Loss = 0.6209\n",
            "Batch 100: Loss = 0.7719\n",
            "Batch 200: Loss = 0.7545\n",
            "Batch 300: Loss = 0.5160\n",
            "Batch 400: Loss = 0.8382\n",
            "Batch 500: Loss = 0.4879\n",
            "Batch 600: Loss = 0.7243\n",
            "Batch 700: Loss = 0.4993\n",
            "Batch 800: Loss = 0.6062\n",
            "Batch 900: Loss = 0.4935\n",
            "Batch 1000: Loss = 0.6233\n",
            "Batch 1100: Loss = 0.5456\n",
            "Batch 1200: Loss = 0.5517\n",
            "Batch 1300: Loss = 0.5326\n",
            "Batch 1400: Loss = 0.6629\n",
            "Batch 1500: Loss = 0.5684\n",
            "Batch 1600: Loss = 0.5800\n",
            "Batch 1700: Loss = 0.5403\n",
            "Batch 1800: Loss = 0.4918\n",
            "Epoch 4/50, Training Loss: 0.6208, Training Accuracy: 86.55%\n",
            "Batch 0: Loss = 0.5345\n",
            "Batch 100: Loss = 0.4433\n",
            "Batch 200: Loss = 0.4695\n",
            "Batch 300: Loss = 0.5658\n",
            "Batch 400: Loss = 0.6418\n",
            "Batch 500: Loss = 0.5356\n",
            "Batch 600: Loss = 0.4652\n",
            "Batch 700: Loss = 0.5717\n",
            "Batch 800: Loss = 0.6773\n",
            "Batch 900: Loss = 0.4811\n",
            "Batch 1000: Loss = 0.5425\n",
            "Batch 1100: Loss = 0.4999\n",
            "Batch 1200: Loss = 0.7137\n",
            "Batch 1300: Loss = 0.5209\n",
            "Batch 1400: Loss = 0.4778\n",
            "Batch 1500: Loss = 0.6630\n",
            "Batch 1600: Loss = 0.5115\n",
            "Batch 1700: Loss = 0.7620\n",
            "Batch 1800: Loss = 0.5229\n",
            "Epoch 5/50, Training Loss: 0.5473, Training Accuracy: 87.81%\n",
            "Batch 0: Loss = 0.4076\n",
            "Batch 100: Loss = 0.5658\n",
            "Batch 200: Loss = 0.5217\n",
            "Batch 300: Loss = 0.4758\n",
            "Batch 400: Loss = 0.3301\n",
            "Batch 500: Loss = 0.5686\n",
            "Batch 600: Loss = 0.6805\n",
            "Batch 700: Loss = 0.6630\n",
            "Batch 800: Loss = 0.3596\n",
            "Batch 900: Loss = 0.5819\n",
            "Batch 1000: Loss = 0.6656\n",
            "Batch 1100: Loss = 0.5510\n",
            "Batch 1200: Loss = 0.5566\n",
            "Batch 1300: Loss = 0.4124\n",
            "Batch 1400: Loss = 0.4398\n",
            "Batch 1500: Loss = 0.3120\n",
            "Batch 1600: Loss = 0.3913\n",
            "Batch 1700: Loss = 0.3530\n",
            "Batch 1800: Loss = 0.3669\n",
            "Epoch 6/50, Training Loss: 0.4954, Training Accuracy: 88.65%\n",
            "Batch 0: Loss = 0.4054\n",
            "Batch 100: Loss = 0.5312\n",
            "Batch 200: Loss = 0.5354\n",
            "Batch 300: Loss = 0.3376\n",
            "Batch 400: Loss = 0.5084\n",
            "Batch 500: Loss = 0.3046\n",
            "Batch 600: Loss = 0.4028\n",
            "Batch 700: Loss = 0.4513\n",
            "Batch 800: Loss = 0.3974\n",
            "Batch 900: Loss = 0.3538\n",
            "Batch 1000: Loss = 0.3166\n",
            "Batch 1100: Loss = 0.4323\n",
            "Batch 1200: Loss = 0.4139\n",
            "Batch 1300: Loss = 0.4255\n",
            "Batch 1400: Loss = 0.2609\n",
            "Batch 1500: Loss = 0.4681\n",
            "Batch 1600: Loss = 0.4697\n",
            "Batch 1700: Loss = 0.2280\n",
            "Batch 1800: Loss = 0.3924\n",
            "Epoch 7/50, Training Loss: 0.4564, Training Accuracy: 89.26%\n",
            "Batch 0: Loss = 0.4530\n",
            "Batch 100: Loss = 0.4010\n",
            "Batch 200: Loss = 0.2844\n",
            "Batch 300: Loss = 0.6690\n",
            "Batch 400: Loss = 0.3571\n",
            "Batch 500: Loss = 0.4827\n",
            "Batch 600: Loss = 0.5719\n",
            "Batch 700: Loss = 0.3581\n",
            "Batch 800: Loss = 0.7231\n",
            "Batch 900: Loss = 0.3626\n",
            "Batch 1000: Loss = 0.3621\n",
            "Batch 1100: Loss = 0.5445\n",
            "Batch 1200: Loss = 0.4675\n",
            "Batch 1300: Loss = 0.5754\n",
            "Batch 1400: Loss = 0.2641\n",
            "Batch 1500: Loss = 0.3979\n",
            "Batch 1600: Loss = 0.2473\n",
            "Batch 1700: Loss = 0.2049\n",
            "Batch 1800: Loss = 0.6982\n",
            "Epoch 8/50, Training Loss: 0.4260, Training Accuracy: 89.83%\n",
            "Batch 0: Loss = 0.3746\n",
            "Batch 100: Loss = 0.3325\n",
            "Batch 200: Loss = 0.3848\n",
            "Batch 300: Loss = 0.3769\n",
            "Batch 400: Loss = 0.4736\n",
            "Batch 500: Loss = 0.2964\n",
            "Batch 600: Loss = 0.2925\n",
            "Batch 700: Loss = 0.3721\n",
            "Batch 800: Loss = 0.3560\n",
            "Batch 900: Loss = 0.5598\n",
            "Batch 1000: Loss = 0.4572\n",
            "Batch 1100: Loss = 0.1600\n",
            "Batch 1200: Loss = 0.4496\n",
            "Batch 1300: Loss = 0.3738\n",
            "Batch 1400: Loss = 0.5584\n",
            "Batch 1500: Loss = 0.2695\n",
            "Batch 1600: Loss = 0.4053\n",
            "Batch 1700: Loss = 0.3782\n",
            "Batch 1800: Loss = 0.2791\n",
            "Epoch 9/50, Training Loss: 0.4014, Training Accuracy: 90.26%\n",
            "Batch 0: Loss = 0.5645\n",
            "Batch 100: Loss = 0.5659\n",
            "Batch 200: Loss = 0.2663\n",
            "Batch 300: Loss = 0.3583\n",
            "Batch 400: Loss = 0.5255\n",
            "Batch 500: Loss = 0.3679\n",
            "Batch 600: Loss = 0.6409\n",
            "Batch 700: Loss = 0.2242\n",
            "Batch 800: Loss = 0.3979\n",
            "Batch 900: Loss = 0.4798\n",
            "Batch 1000: Loss = 0.4616\n",
            "Batch 1100: Loss = 0.5917\n",
            "Batch 1200: Loss = 0.4287\n",
            "Batch 1300: Loss = 0.3759\n",
            "Batch 1400: Loss = 0.2336\n",
            "Batch 1500: Loss = 0.2863\n",
            "Batch 1600: Loss = 0.3322\n",
            "Batch 1700: Loss = 0.3599\n",
            "Batch 1800: Loss = 0.6002\n",
            "Epoch 10/50, Training Loss: 0.3811, Training Accuracy: 90.63%\n",
            "Batch 0: Loss = 0.5091\n",
            "Batch 100: Loss = 0.5095\n",
            "Batch 200: Loss = 0.2408\n",
            "Batch 300: Loss = 0.5373\n",
            "Batch 400: Loss = 0.4320\n",
            "Batch 500: Loss = 0.3284\n",
            "Batch 600: Loss = 0.4252\n",
            "Batch 700: Loss = 0.4768\n",
            "Batch 800: Loss = 0.2476\n",
            "Batch 900: Loss = 0.5052\n",
            "Batch 1000: Loss = 0.5861\n",
            "Batch 1100: Loss = 0.5195\n",
            "Batch 1200: Loss = 0.3360\n",
            "Batch 1300: Loss = 0.3232\n",
            "Batch 1400: Loss = 0.2244\n",
            "Batch 1500: Loss = 0.2799\n",
            "Batch 1600: Loss = 0.2861\n",
            "Batch 1700: Loss = 0.4543\n",
            "Batch 1800: Loss = 0.2968\n",
            "Epoch 11/50, Training Loss: 0.3639, Training Accuracy: 90.90%\n",
            "Batch 0: Loss = 0.3892\n",
            "Batch 100: Loss = 0.5478\n",
            "Batch 200: Loss = 0.3744\n",
            "Batch 300: Loss = 0.4444\n",
            "Batch 400: Loss = 0.3415\n",
            "Batch 500: Loss = 0.2665\n",
            "Batch 600: Loss = 0.3358\n",
            "Batch 700: Loss = 0.2131\n",
            "Batch 800: Loss = 0.2147\n",
            "Batch 900: Loss = 0.2142\n",
            "Batch 1000: Loss = 0.3814\n",
            "Batch 1100: Loss = 0.3233\n",
            "Batch 1200: Loss = 0.3693\n",
            "Batch 1300: Loss = 0.2014\n",
            "Batch 1400: Loss = 0.3757\n",
            "Batch 1500: Loss = 0.1299\n",
            "Batch 1600: Loss = 0.3927\n",
            "Batch 1700: Loss = 0.4059\n",
            "Batch 1800: Loss = 0.4087\n",
            "Epoch 12/50, Training Loss: 0.3491, Training Accuracy: 91.17%\n",
            "Batch 0: Loss = 0.6415\n",
            "Batch 100: Loss = 0.3828\n",
            "Batch 200: Loss = 0.5419\n",
            "Batch 300: Loss = 0.3525\n",
            "Batch 400: Loss = 0.5065\n",
            "Batch 500: Loss = 0.4986\n",
            "Batch 600: Loss = 0.2947\n",
            "Batch 700: Loss = 0.2382\n",
            "Batch 800: Loss = 0.2522\n",
            "Batch 900: Loss = 0.2212\n",
            "Batch 1000: Loss = 0.2253\n",
            "Batch 1100: Loss = 0.3611\n",
            "Batch 1200: Loss = 0.3009\n",
            "Batch 1300: Loss = 0.2376\n",
            "Batch 1400: Loss = 0.2174\n",
            "Batch 1500: Loss = 0.5016\n",
            "Batch 1600: Loss = 0.3589\n",
            "Batch 1700: Loss = 0.4064\n",
            "Batch 1800: Loss = 0.1277\n",
            "Epoch 13/50, Training Loss: 0.3361, Training Accuracy: 91.42%\n",
            "Batch 0: Loss = 0.4494\n",
            "Batch 100: Loss = 0.3675\n",
            "Batch 200: Loss = 0.3749\n",
            "Batch 300: Loss = 0.1662\n",
            "Batch 400: Loss = 0.2810\n",
            "Batch 500: Loss = 0.2885\n",
            "Batch 600: Loss = 0.3742\n",
            "Batch 700: Loss = 0.3632\n",
            "Batch 800: Loss = 0.3877\n",
            "Batch 900: Loss = 0.4313\n",
            "Batch 1000: Loss = 0.4721\n",
            "Batch 1100: Loss = 0.2495\n",
            "Batch 1200: Loss = 0.3974\n",
            "Batch 1300: Loss = 0.3267\n",
            "Batch 1400: Loss = 0.4361\n",
            "Batch 1500: Loss = 0.3170\n",
            "Batch 1600: Loss = 0.2009\n",
            "Batch 1700: Loss = 0.3221\n",
            "Batch 1800: Loss = 0.5667\n",
            "Epoch 14/50, Training Loss: 0.3246, Training Accuracy: 91.64%\n",
            "Batch 0: Loss = 0.3425\n",
            "Batch 100: Loss = 0.1746\n",
            "Batch 200: Loss = 0.1543\n",
            "Batch 300: Loss = 0.1702\n",
            "Batch 400: Loss = 0.3347\n",
            "Batch 500: Loss = 0.2964\n",
            "Batch 600: Loss = 0.3895\n",
            "Batch 700: Loss = 0.6326\n",
            "Batch 800: Loss = 0.5635\n",
            "Batch 900: Loss = 0.5636\n",
            "Batch 1000: Loss = 0.2467\n",
            "Batch 1100: Loss = 0.2698\n",
            "Batch 1200: Loss = 0.1817\n",
            "Batch 1300: Loss = 0.2727\n",
            "Batch 1400: Loss = 0.3607\n",
            "Batch 1500: Loss = 0.2911\n",
            "Batch 1600: Loss = 0.2842\n",
            "Batch 1700: Loss = 0.2379\n",
            "Batch 1800: Loss = 0.4119\n",
            "Epoch 15/50, Training Loss: 0.3143, Training Accuracy: 91.86%\n",
            "Batch 0: Loss = 0.1673\n",
            "Batch 100: Loss = 0.2615\n",
            "Batch 200: Loss = 0.2528\n",
            "Batch 300: Loss = 0.1918\n",
            "Batch 400: Loss = 0.4473\n",
            "Batch 500: Loss = 0.1938\n",
            "Batch 600: Loss = 0.3646\n",
            "Batch 700: Loss = 0.2915\n",
            "Batch 800: Loss = 0.2098\n",
            "Batch 900: Loss = 0.2468\n",
            "Batch 1000: Loss = 0.2680\n",
            "Batch 1100: Loss = 0.3314\n",
            "Batch 1200: Loss = 0.3174\n",
            "Batch 1300: Loss = 0.2462\n",
            "Batch 1400: Loss = 0.2276\n",
            "Batch 1500: Loss = 0.1894\n",
            "Batch 1600: Loss = 0.1273\n",
            "Batch 1700: Loss = 0.1440\n",
            "Batch 1800: Loss = 0.2696\n",
            "Epoch 16/50, Training Loss: 0.3050, Training Accuracy: 92.05%\n",
            "Batch 0: Loss = 0.2653\n",
            "Batch 100: Loss = 0.1823\n",
            "Batch 200: Loss = 0.2656\n",
            "Batch 300: Loss = 0.1860\n",
            "Batch 400: Loss = 0.3697\n",
            "Batch 500: Loss = 0.5209\n",
            "Batch 600: Loss = 0.3229\n",
            "Batch 700: Loss = 0.2264\n",
            "Batch 800: Loss = 0.4274\n",
            "Batch 900: Loss = 0.3617\n",
            "Batch 1000: Loss = 0.2645\n",
            "Batch 1100: Loss = 0.2519\n",
            "Batch 1200: Loss = 0.1686\n",
            "Batch 1300: Loss = 0.4002\n",
            "Batch 1400: Loss = 0.1651\n",
            "Batch 1500: Loss = 0.2272\n",
            "Batch 1600: Loss = 0.2140\n",
            "Batch 1700: Loss = 0.5010\n",
            "Batch 1800: Loss = 0.3578\n",
            "Epoch 17/50, Training Loss: 0.2965, Training Accuracy: 92.27%\n",
            "Batch 0: Loss = 0.2411\n",
            "Batch 100: Loss = 0.3810\n",
            "Batch 200: Loss = 0.4583\n",
            "Batch 300: Loss = 0.3636\n",
            "Batch 400: Loss = 0.2840\n",
            "Batch 500: Loss = 0.3058\n",
            "Batch 600: Loss = 0.3747\n",
            "Batch 700: Loss = 0.5575\n",
            "Batch 800: Loss = 0.5151\n",
            "Batch 900: Loss = 0.1734\n",
            "Batch 1000: Loss = 0.3395\n",
            "Batch 1100: Loss = 0.2345\n",
            "Batch 1200: Loss = 0.2761\n",
            "Batch 1300: Loss = 0.2578\n",
            "Batch 1400: Loss = 0.1923\n",
            "Batch 1500: Loss = 0.1427\n",
            "Batch 1600: Loss = 0.2050\n",
            "Batch 1700: Loss = 0.3920\n",
            "Batch 1800: Loss = 0.3247\n",
            "Epoch 18/50, Training Loss: 0.2886, Training Accuracy: 92.41%\n",
            "Batch 0: Loss = 0.2511\n",
            "Batch 100: Loss = 0.4477\n",
            "Batch 200: Loss = 0.3754\n",
            "Batch 300: Loss = 0.3035\n",
            "Batch 400: Loss = 0.1655\n",
            "Batch 500: Loss = 0.2631\n",
            "Batch 600: Loss = 0.2082\n",
            "Batch 700: Loss = 0.1488\n",
            "Batch 800: Loss = 0.3798\n",
            "Batch 900: Loss = 0.2606\n",
            "Batch 1000: Loss = 0.1984\n",
            "Batch 1100: Loss = 0.2087\n",
            "Batch 1200: Loss = 0.1981\n",
            "Batch 1300: Loss = 0.2674\n",
            "Batch 1400: Loss = 0.3794\n",
            "Batch 1500: Loss = 0.2978\n",
            "Batch 1600: Loss = 0.1043\n",
            "Batch 1700: Loss = 0.1515\n",
            "Batch 1800: Loss = 0.1924\n",
            "Epoch 19/50, Training Loss: 0.2814, Training Accuracy: 92.59%\n",
            "Batch 0: Loss = 0.3614\n",
            "Batch 100: Loss = 0.1952\n",
            "Batch 200: Loss = 0.2031\n",
            "Batch 300: Loss = 0.3551\n",
            "Batch 400: Loss = 0.2300\n",
            "Batch 500: Loss = 0.2931\n",
            "Batch 600: Loss = 0.2622\n",
            "Batch 700: Loss = 0.1815\n",
            "Batch 800: Loss = 0.1921\n",
            "Batch 900: Loss = 0.1797\n",
            "Batch 1000: Loss = 0.2661\n",
            "Batch 1100: Loss = 0.2582\n",
            "Batch 1200: Loss = 0.1952\n",
            "Batch 1300: Loss = 0.2250\n",
            "Batch 1400: Loss = 0.2256\n",
            "Batch 1500: Loss = 0.2961\n",
            "Batch 1600: Loss = 0.4310\n",
            "Batch 1700: Loss = 0.3509\n",
            "Batch 1800: Loss = 0.2163\n",
            "Epoch 20/50, Training Loss: 0.2747, Training Accuracy: 92.77%\n",
            "Batch 0: Loss = 0.2945\n",
            "Batch 100: Loss = 0.1923\n",
            "Batch 200: Loss = 0.1517\n",
            "Batch 300: Loss = 0.3265\n",
            "Batch 400: Loss = 0.2126\n",
            "Batch 500: Loss = 0.5318\n",
            "Batch 600: Loss = 0.2918\n",
            "Batch 700: Loss = 0.2746\n",
            "Batch 800: Loss = 0.1762\n",
            "Batch 900: Loss = 0.0957\n",
            "Batch 1000: Loss = 0.1712\n",
            "Batch 1100: Loss = 0.2972\n",
            "Batch 1200: Loss = 0.1839\n",
            "Batch 1300: Loss = 0.4144\n",
            "Batch 1400: Loss = 0.1587\n",
            "Batch 1500: Loss = 0.1670\n",
            "Batch 1600: Loss = 0.2524\n",
            "Batch 1700: Loss = 0.2831\n",
            "Batch 1800: Loss = 0.2551\n",
            "Epoch 21/50, Training Loss: 0.2684, Training Accuracy: 92.95%\n",
            "Batch 0: Loss = 0.2773\n",
            "Batch 100: Loss = 0.3957\n",
            "Batch 200: Loss = 0.2634\n",
            "Batch 300: Loss = 0.2133\n",
            "Batch 400: Loss = 0.4266\n",
            "Batch 500: Loss = 0.2680\n",
            "Batch 600: Loss = 0.3826\n",
            "Batch 700: Loss = 0.0878\n",
            "Batch 800: Loss = 0.3022\n",
            "Batch 900: Loss = 0.2194\n",
            "Batch 1000: Loss = 0.2376\n",
            "Batch 1100: Loss = 0.2832\n",
            "Batch 1200: Loss = 0.4098\n",
            "Batch 1300: Loss = 0.2640\n",
            "Batch 1400: Loss = 0.1716\n",
            "Batch 1500: Loss = 0.2996\n",
            "Batch 1600: Loss = 0.1411\n",
            "Batch 1700: Loss = 0.3378\n",
            "Batch 1800: Loss = 0.2868\n",
            "Epoch 22/50, Training Loss: 0.2625, Training Accuracy: 93.06%\n",
            "Batch 0: Loss = 0.2852\n",
            "Batch 100: Loss = 0.0903\n",
            "Batch 200: Loss = 0.3874\n",
            "Batch 300: Loss = 0.2498\n",
            "Batch 400: Loss = 0.2752\n",
            "Batch 500: Loss = 0.1794\n",
            "Batch 600: Loss = 0.4341\n",
            "Batch 700: Loss = 0.5292\n",
            "Batch 800: Loss = 0.2692\n",
            "Batch 900: Loss = 0.1644\n",
            "Batch 1000: Loss = 0.2413\n",
            "Batch 1100: Loss = 0.1352\n",
            "Batch 1200: Loss = 0.3647\n",
            "Batch 1300: Loss = 0.6704\n",
            "Batch 1400: Loss = 0.4308\n",
            "Batch 1500: Loss = 0.1228\n",
            "Batch 1600: Loss = 0.1474\n",
            "Batch 1700: Loss = 0.2605\n",
            "Batch 1800: Loss = 0.2317\n",
            "Epoch 23/50, Training Loss: 0.2569, Training Accuracy: 93.19%\n",
            "Batch 0: Loss = 0.1171\n",
            "Batch 100: Loss = 0.2026\n",
            "Batch 200: Loss = 0.1316\n",
            "Batch 300: Loss = 0.1547\n",
            "Batch 400: Loss = 0.2657\n",
            "Batch 500: Loss = 0.1826\n",
            "Batch 600: Loss = 0.1396\n",
            "Batch 700: Loss = 0.2191\n",
            "Batch 800: Loss = 0.1690\n",
            "Batch 900: Loss = 0.3505\n",
            "Batch 1000: Loss = 0.1749\n",
            "Batch 1100: Loss = 0.2219\n",
            "Batch 1200: Loss = 0.3024\n",
            "Batch 1300: Loss = 0.3471\n",
            "Batch 1400: Loss = 0.2072\n",
            "Batch 1500: Loss = 0.1632\n",
            "Batch 1600: Loss = 0.1450\n",
            "Batch 1700: Loss = 0.2127\n",
            "Batch 1800: Loss = 0.2104\n",
            "Epoch 24/50, Training Loss: 0.2516, Training Accuracy: 93.29%\n",
            "Batch 0: Loss = 0.2365\n",
            "Batch 100: Loss = 0.2533\n",
            "Batch 200: Loss = 0.0675\n",
            "Batch 300: Loss = 0.2129\n",
            "Batch 400: Loss = 0.1789\n",
            "Batch 500: Loss = 0.2172\n",
            "Batch 600: Loss = 0.1351\n",
            "Batch 700: Loss = 0.1249\n",
            "Batch 800: Loss = 0.3703\n",
            "Batch 900: Loss = 0.2050\n",
            "Batch 1000: Loss = 0.1052\n",
            "Batch 1100: Loss = 0.4091\n",
            "Batch 1200: Loss = 0.1342\n",
            "Batch 1300: Loss = 0.2596\n",
            "Batch 1400: Loss = 0.1565\n",
            "Batch 1500: Loss = 0.1367\n",
            "Batch 1600: Loss = 0.5959\n",
            "Batch 1700: Loss = 0.4166\n",
            "Batch 1800: Loss = 0.3140\n",
            "Epoch 25/50, Training Loss: 0.2467, Training Accuracy: 93.39%\n",
            "Batch 0: Loss = 0.1230\n",
            "Batch 100: Loss = 0.2205\n",
            "Batch 200: Loss = 0.2645\n",
            "Batch 300: Loss = 0.1803\n",
            "Batch 400: Loss = 0.1735\n",
            "Batch 500: Loss = 0.2051\n",
            "Batch 600: Loss = 0.3251\n",
            "Batch 700: Loss = 0.2091\n",
            "Batch 800: Loss = 0.2513\n",
            "Batch 900: Loss = 0.1794\n",
            "Batch 1000: Loss = 0.3108\n",
            "Batch 1100: Loss = 0.2757\n",
            "Batch 1200: Loss = 0.0851\n",
            "Batch 1300: Loss = 0.1496\n",
            "Batch 1400: Loss = 0.4091\n",
            "Batch 1500: Loss = 0.1887\n",
            "Batch 1600: Loss = 0.1762\n",
            "Batch 1700: Loss = 0.2096\n",
            "Batch 1800: Loss = 0.4081\n",
            "Epoch 26/50, Training Loss: 0.2419, Training Accuracy: 93.50%\n",
            "Batch 0: Loss = 0.1599\n",
            "Batch 100: Loss = 0.4851\n",
            "Batch 200: Loss = 0.1493\n",
            "Batch 300: Loss = 0.1634\n",
            "Batch 400: Loss = 0.3401\n",
            "Batch 500: Loss = 0.3942\n",
            "Batch 600: Loss = 0.1826\n",
            "Batch 700: Loss = 0.1974\n",
            "Batch 800: Loss = 0.2519\n",
            "Batch 900: Loss = 0.1322\n",
            "Batch 1000: Loss = 0.1729\n",
            "Batch 1100: Loss = 0.2741\n",
            "Batch 1200: Loss = 0.3953\n",
            "Batch 1300: Loss = 0.1943\n",
            "Batch 1400: Loss = 0.2757\n",
            "Batch 1500: Loss = 0.1728\n",
            "Batch 1600: Loss = 0.2534\n",
            "Batch 1700: Loss = 0.1237\n",
            "Batch 1800: Loss = 0.2107\n",
            "Epoch 27/50, Training Loss: 0.2374, Training Accuracy: 93.61%\n",
            "Batch 0: Loss = 0.1503\n",
            "Batch 100: Loss = 0.1393\n",
            "Batch 200: Loss = 0.2126\n",
            "Batch 300: Loss = 0.3489\n",
            "Batch 400: Loss = 0.1787\n",
            "Batch 500: Loss = 0.2375\n",
            "Batch 600: Loss = 0.1501\n",
            "Batch 700: Loss = 0.1040\n",
            "Batch 800: Loss = 0.4049\n",
            "Batch 900: Loss = 0.2735\n",
            "Batch 1000: Loss = 0.4821\n",
            "Batch 1100: Loss = 0.2424\n",
            "Batch 1200: Loss = 0.1674\n",
            "Batch 1300: Loss = 0.0520\n",
            "Batch 1400: Loss = 0.4119\n",
            "Batch 1500: Loss = 0.3534\n",
            "Batch 1600: Loss = 0.1739\n",
            "Batch 1700: Loss = 0.2797\n",
            "Batch 1800: Loss = 0.2449\n",
            "Epoch 28/50, Training Loss: 0.2331, Training Accuracy: 93.71%\n",
            "Batch 0: Loss = 0.1407\n",
            "Batch 100: Loss = 0.1653\n",
            "Batch 200: Loss = 0.4063\n",
            "Batch 300: Loss = 0.1613\n",
            "Batch 400: Loss = 0.2040\n",
            "Batch 500: Loss = 0.2448\n",
            "Batch 600: Loss = 0.0740\n",
            "Batch 700: Loss = 0.2525\n",
            "Batch 800: Loss = 0.1099\n",
            "Batch 900: Loss = 0.1985\n",
            "Batch 1000: Loss = 0.1450\n",
            "Batch 1100: Loss = 0.2091\n",
            "Batch 1200: Loss = 0.2655\n",
            "Batch 1300: Loss = 0.1141\n",
            "Batch 1400: Loss = 0.2157\n",
            "Batch 1500: Loss = 0.2706\n",
            "Batch 1600: Loss = 0.2054\n",
            "Batch 1700: Loss = 0.2335\n",
            "Batch 1800: Loss = 0.2251\n",
            "Epoch 29/50, Training Loss: 0.2290, Training Accuracy: 93.84%\n",
            "Batch 0: Loss = 0.0977\n",
            "Batch 100: Loss = 0.1779\n",
            "Batch 200: Loss = 0.5031\n",
            "Batch 300: Loss = 0.2820\n",
            "Batch 400: Loss = 0.1674\n",
            "Batch 500: Loss = 0.1046\n",
            "Batch 600: Loss = 0.1536\n",
            "Batch 700: Loss = 0.0699\n",
            "Batch 800: Loss = 0.2225\n",
            "Batch 900: Loss = 0.0675\n",
            "Batch 1000: Loss = 0.0866\n",
            "Batch 1100: Loss = 0.0951\n",
            "Batch 1200: Loss = 0.2007\n",
            "Batch 1300: Loss = 0.1291\n",
            "Batch 1400: Loss = 0.1192\n",
            "Batch 1500: Loss = 0.2084\n",
            "Batch 1600: Loss = 0.2573\n",
            "Batch 1700: Loss = 0.2111\n",
            "Batch 1800: Loss = 0.1511\n",
            "Epoch 30/50, Training Loss: 0.2251, Training Accuracy: 93.94%\n",
            "Batch 0: Loss = 0.2189\n",
            "Batch 100: Loss = 0.3564\n",
            "Batch 200: Loss = 0.0766\n",
            "Batch 300: Loss = 0.2190\n",
            "Batch 400: Loss = 0.2440\n",
            "Batch 500: Loss = 0.3435\n",
            "Batch 600: Loss = 0.1637\n",
            "Batch 700: Loss = 0.3344\n",
            "Batch 800: Loss = 0.2367\n",
            "Batch 900: Loss = 0.3321\n",
            "Batch 1000: Loss = 0.2537\n",
            "Batch 1100: Loss = 0.1063\n",
            "Batch 1200: Loss = 0.3074\n",
            "Batch 1300: Loss = 0.2147\n",
            "Batch 1400: Loss = 0.1894\n",
            "Batch 1500: Loss = 0.3145\n",
            "Batch 1600: Loss = 0.1726\n",
            "Batch 1700: Loss = 0.3171\n",
            "Batch 1800: Loss = 0.1237\n",
            "Epoch 31/50, Training Loss: 0.2213, Training Accuracy: 94.03%\n",
            "Batch 0: Loss = 0.4198\n",
            "Batch 100: Loss = 0.2350\n",
            "Batch 200: Loss = 0.3193\n",
            "Batch 300: Loss = 0.1774\n",
            "Batch 400: Loss = 0.2553\n",
            "Batch 500: Loss = 0.2278\n",
            "Batch 600: Loss = 0.1566\n",
            "Batch 700: Loss = 0.0613\n",
            "Batch 800: Loss = 0.1787\n",
            "Batch 900: Loss = 0.1361\n",
            "Batch 1000: Loss = 0.1162\n",
            "Batch 1100: Loss = 0.2364\n",
            "Batch 1200: Loss = 0.5073\n",
            "Batch 1300: Loss = 0.0730\n",
            "Batch 1400: Loss = 0.1921\n",
            "Batch 1500: Loss = 0.1308\n",
            "Batch 1600: Loss = 0.1983\n",
            "Batch 1700: Loss = 0.2739\n",
            "Batch 1800: Loss = 0.1956\n",
            "Epoch 32/50, Training Loss: 0.2177, Training Accuracy: 94.12%\n",
            "Batch 0: Loss = 0.2228\n",
            "Batch 100: Loss = 0.1408\n",
            "Batch 200: Loss = 0.1584\n",
            "Batch 300: Loss = 0.2818\n",
            "Batch 400: Loss = 0.2931\n",
            "Batch 500: Loss = 0.1148\n",
            "Batch 600: Loss = 0.3169\n",
            "Batch 700: Loss = 0.2712\n",
            "Batch 800: Loss = 0.2501\n",
            "Batch 900: Loss = 0.1589\n",
            "Batch 1000: Loss = 0.2058\n",
            "Batch 1100: Loss = 0.2093\n",
            "Batch 1200: Loss = 0.1548\n",
            "Batch 1300: Loss = 0.1006\n",
            "Batch 1400: Loss = 0.2856\n",
            "Batch 1500: Loss = 0.2027\n",
            "Batch 1600: Loss = 0.0589\n",
            "Batch 1700: Loss = 0.2184\n",
            "Batch 1800: Loss = 0.2107\n",
            "Epoch 33/50, Training Loss: 0.2142, Training Accuracy: 94.23%\n",
            "Batch 0: Loss = 0.1985\n",
            "Batch 100: Loss = 0.2034\n",
            "Batch 200: Loss = 0.3810\n",
            "Batch 300: Loss = 0.5522\n",
            "Batch 400: Loss = 0.4579\n",
            "Batch 500: Loss = 0.1614\n",
            "Batch 600: Loss = 0.4660\n",
            "Batch 700: Loss = 0.0660\n",
            "Batch 800: Loss = 0.3103\n",
            "Batch 900: Loss = 0.4108\n",
            "Batch 1000: Loss = 0.1971\n",
            "Batch 1100: Loss = 0.1744\n",
            "Batch 1200: Loss = 0.2185\n",
            "Batch 1300: Loss = 0.4271\n",
            "Batch 1400: Loss = 0.3433\n",
            "Batch 1500: Loss = 0.2234\n",
            "Batch 1600: Loss = 0.3245\n",
            "Batch 1700: Loss = 0.1721\n",
            "Batch 1800: Loss = 0.1120\n",
            "Epoch 34/50, Training Loss: 0.2109, Training Accuracy: 94.32%\n",
            "Batch 0: Loss = 0.0878\n",
            "Batch 100: Loss = 0.2736\n",
            "Batch 200: Loss = 0.1882\n",
            "Batch 300: Loss = 0.3219\n",
            "Batch 400: Loss = 0.2273\n",
            "Batch 500: Loss = 0.2587\n",
            "Batch 600: Loss = 0.2425\n",
            "Batch 700: Loss = 0.2025\n",
            "Batch 800: Loss = 0.2719\n",
            "Batch 900: Loss = 0.1969\n",
            "Batch 1000: Loss = 0.1835\n",
            "Batch 1100: Loss = 0.1300\n",
            "Batch 1200: Loss = 0.1969\n",
            "Batch 1300: Loss = 0.2086\n",
            "Batch 1400: Loss = 0.2190\n",
            "Batch 1500: Loss = 0.1112\n",
            "Batch 1600: Loss = 0.2632\n",
            "Batch 1700: Loss = 0.1726\n",
            "Batch 1800: Loss = 0.1275\n",
            "Epoch 35/50, Training Loss: 0.2076, Training Accuracy: 94.40%\n",
            "Batch 0: Loss = 0.2899\n",
            "Batch 100: Loss = 0.0778\n",
            "Batch 200: Loss = 0.1797\n",
            "Batch 300: Loss = 0.2469\n",
            "Batch 400: Loss = 0.3401\n",
            "Batch 500: Loss = 0.1807\n",
            "Batch 600: Loss = 0.1859\n",
            "Batch 700: Loss = 0.3147\n",
            "Batch 800: Loss = 0.2646\n",
            "Batch 900: Loss = 0.1087\n",
            "Batch 1000: Loss = 0.1648\n",
            "Batch 1100: Loss = 0.1809\n",
            "Batch 1200: Loss = 0.3170\n",
            "Batch 1300: Loss = 0.0849\n",
            "Batch 1400: Loss = 0.2948\n",
            "Batch 1500: Loss = 0.2358\n",
            "Batch 1600: Loss = 0.1695\n",
            "Batch 1700: Loss = 0.1533\n",
            "Batch 1800: Loss = 0.3643\n",
            "Epoch 36/50, Training Loss: 0.2045, Training Accuracy: 94.48%\n",
            "Batch 0: Loss = 0.0993\n",
            "Batch 100: Loss = 0.2092\n",
            "Batch 200: Loss = 0.1717\n",
            "Batch 300: Loss = 0.0924\n",
            "Batch 400: Loss = 0.0990\n",
            "Batch 500: Loss = 0.1578\n",
            "Batch 600: Loss = 0.3051\n",
            "Batch 700: Loss = 0.1913\n",
            "Batch 800: Loss = 0.1231\n",
            "Batch 900: Loss = 0.2798\n",
            "Batch 1000: Loss = 0.3223\n",
            "Batch 1100: Loss = 0.0951\n",
            "Batch 1200: Loss = 0.3363\n",
            "Batch 1300: Loss = 0.1440\n",
            "Batch 1400: Loss = 0.1293\n",
            "Batch 1500: Loss = 0.3816\n",
            "Batch 1600: Loss = 0.1489\n",
            "Batch 1700: Loss = 0.2204\n",
            "Batch 1800: Loss = 0.2603\n",
            "Epoch 37/50, Training Loss: 0.2015, Training Accuracy: 94.57%\n",
            "Batch 0: Loss = 0.2074\n",
            "Batch 100: Loss = 0.2366\n",
            "Batch 200: Loss = 0.2312\n",
            "Batch 300: Loss = 0.1064\n",
            "Batch 400: Loss = 0.1245\n",
            "Batch 500: Loss = 0.3227\n",
            "Batch 600: Loss = 0.0798\n",
            "Batch 700: Loss = 0.1658\n",
            "Batch 800: Loss = 0.0891\n",
            "Batch 900: Loss = 0.2314\n",
            "Batch 1000: Loss = 0.2152\n",
            "Batch 1100: Loss = 0.2977\n",
            "Batch 1200: Loss = 0.1525\n",
            "Batch 1300: Loss = 0.0940\n",
            "Batch 1400: Loss = 0.4263\n",
            "Batch 1500: Loss = 0.1436\n",
            "Batch 1600: Loss = 0.4211\n",
            "Batch 1700: Loss = 0.0533\n",
            "Batch 1800: Loss = 0.2702\n",
            "Epoch 38/50, Training Loss: 0.1985, Training Accuracy: 94.61%\n",
            "Batch 0: Loss = 0.1988\n",
            "Batch 100: Loss = 0.1120\n",
            "Batch 200: Loss = 0.1467\n",
            "Batch 300: Loss = 0.2042\n",
            "Batch 400: Loss = 0.3599\n",
            "Batch 500: Loss = 0.1392\n",
            "Batch 600: Loss = 0.2623\n",
            "Batch 700: Loss = 0.1613\n",
            "Batch 800: Loss = 0.1379\n",
            "Batch 900: Loss = 0.0385\n",
            "Batch 1000: Loss = 0.3303\n",
            "Batch 1100: Loss = 0.1939\n",
            "Batch 1200: Loss = 0.1790\n",
            "Batch 1300: Loss = 0.1671\n",
            "Batch 1400: Loss = 0.2895\n",
            "Batch 1500: Loss = 0.0782\n",
            "Batch 1600: Loss = 0.0583\n",
            "Batch 1700: Loss = 0.3534\n",
            "Batch 1800: Loss = 0.1893\n",
            "Epoch 39/50, Training Loss: 0.1957, Training Accuracy: 94.71%\n",
            "Batch 0: Loss = 0.1712\n",
            "Batch 100: Loss = 0.6039\n",
            "Batch 200: Loss = 0.3231\n",
            "Batch 300: Loss = 0.1412\n",
            "Batch 400: Loss = 0.1963\n",
            "Batch 500: Loss = 0.1121\n",
            "Batch 600: Loss = 0.2666\n",
            "Batch 700: Loss = 0.2616\n",
            "Batch 800: Loss = 0.1914\n",
            "Batch 900: Loss = 0.1099\n",
            "Batch 1000: Loss = 0.2553\n",
            "Batch 1100: Loss = 0.1067\n",
            "Batch 1200: Loss = 0.1127\n",
            "Batch 1300: Loss = 0.1361\n",
            "Batch 1400: Loss = 0.1099\n",
            "Batch 1500: Loss = 0.2378\n",
            "Batch 1600: Loss = 0.1833\n",
            "Batch 1700: Loss = 0.1394\n",
            "Batch 1800: Loss = 0.4447\n",
            "Epoch 40/50, Training Loss: 0.1929, Training Accuracy: 94.78%\n",
            "Batch 0: Loss = 0.2190\n",
            "Batch 100: Loss = 0.0950\n",
            "Batch 200: Loss = 0.2672\n",
            "Batch 300: Loss = 0.2629\n",
            "Batch 400: Loss = 0.1656\n",
            "Batch 500: Loss = 0.2961\n",
            "Batch 600: Loss = 0.1792\n",
            "Batch 700: Loss = 0.1947\n",
            "Batch 800: Loss = 0.1961\n",
            "Batch 900: Loss = 0.0552\n",
            "Batch 1000: Loss = 0.1069\n",
            "Batch 1100: Loss = 0.1074\n",
            "Batch 1200: Loss = 0.1483\n",
            "Batch 1300: Loss = 0.1562\n",
            "Batch 1400: Loss = 0.2037\n",
            "Batch 1500: Loss = 0.2631\n",
            "Batch 1600: Loss = 0.1024\n",
            "Batch 1700: Loss = 0.2303\n",
            "Batch 1800: Loss = 0.1375\n",
            "Epoch 41/50, Training Loss: 0.1903, Training Accuracy: 94.85%\n",
            "Batch 0: Loss = 0.3303\n",
            "Batch 100: Loss = 0.4939\n",
            "Batch 200: Loss = 0.1596\n",
            "Batch 300: Loss = 0.3093\n",
            "Batch 400: Loss = 0.0922\n",
            "Batch 500: Loss = 0.1518\n",
            "Batch 600: Loss = 0.0920\n",
            "Batch 700: Loss = 0.3186\n",
            "Batch 800: Loss = 0.2128\n",
            "Batch 900: Loss = 0.2656\n",
            "Batch 1000: Loss = 0.1572\n",
            "Batch 1100: Loss = 0.1265\n",
            "Batch 1200: Loss = 0.1524\n",
            "Batch 1300: Loss = 0.1981\n",
            "Batch 1400: Loss = 0.2425\n",
            "Batch 1500: Loss = 0.3641\n",
            "Batch 1600: Loss = 0.1219\n",
            "Batch 1700: Loss = 0.2305\n",
            "Batch 1800: Loss = 0.1987\n",
            "Epoch 42/50, Training Loss: 0.1877, Training Accuracy: 94.94%\n",
            "Batch 0: Loss = 0.1896\n",
            "Batch 100: Loss = 0.0847\n",
            "Batch 200: Loss = 0.2398\n",
            "Batch 300: Loss = 0.2203\n",
            "Batch 400: Loss = 0.1192\n",
            "Batch 500: Loss = 0.1314\n",
            "Batch 600: Loss = 0.0925\n",
            "Batch 700: Loss = 0.1299\n",
            "Batch 800: Loss = 0.1574\n",
            "Batch 900: Loss = 0.2477\n",
            "Batch 1000: Loss = 0.1488\n",
            "Batch 1100: Loss = 0.2023\n",
            "Batch 1200: Loss = 0.0904\n",
            "Batch 1300: Loss = 0.1509\n",
            "Batch 1400: Loss = 0.2307\n",
            "Batch 1500: Loss = 0.2316\n",
            "Batch 1600: Loss = 0.2840\n",
            "Batch 1700: Loss = 0.2979\n",
            "Batch 1800: Loss = 0.2310\n",
            "Epoch 43/50, Training Loss: 0.1852, Training Accuracy: 95.01%\n",
            "Batch 0: Loss = 0.3332\n",
            "Batch 100: Loss = 0.3151\n",
            "Batch 200: Loss = 0.1230\n",
            "Batch 300: Loss = 0.0873\n",
            "Batch 400: Loss = 0.1966\n",
            "Batch 500: Loss = 0.1915\n",
            "Batch 600: Loss = 0.1731\n",
            "Batch 700: Loss = 0.1859\n",
            "Batch 800: Loss = 0.1704\n",
            "Batch 900: Loss = 0.3350\n",
            "Batch 1000: Loss = 0.3798\n",
            "Batch 1100: Loss = 0.2307\n",
            "Batch 1200: Loss = 0.0963\n",
            "Batch 1300: Loss = 0.2055\n",
            "Batch 1400: Loss = 0.2214\n",
            "Batch 1500: Loss = 0.0941\n",
            "Batch 1600: Loss = 0.2451\n",
            "Batch 1700: Loss = 0.1483\n",
            "Batch 1800: Loss = 0.1524\n",
            "Epoch 44/50, Training Loss: 0.1828, Training Accuracy: 95.07%\n",
            "Batch 0: Loss = 0.3785\n",
            "Batch 100: Loss = 0.1016\n",
            "Batch 200: Loss = 0.1083\n",
            "Batch 300: Loss = 0.0939\n",
            "Batch 400: Loss = 0.2016\n",
            "Batch 500: Loss = 0.2331\n",
            "Batch 600: Loss = 0.1196\n",
            "Batch 700: Loss = 0.0504\n",
            "Batch 800: Loss = 0.2571\n",
            "Batch 900: Loss = 0.1550\n",
            "Batch 1000: Loss = 0.2649\n",
            "Batch 1100: Loss = 0.2558\n",
            "Batch 1200: Loss = 0.2909\n",
            "Batch 1300: Loss = 0.1799\n",
            "Batch 1400: Loss = 0.1849\n",
            "Batch 1500: Loss = 0.2541\n",
            "Batch 1600: Loss = 0.1378\n",
            "Batch 1700: Loss = 0.2305\n",
            "Batch 1800: Loss = 0.1813\n",
            "Epoch 45/50, Training Loss: 0.1804, Training Accuracy: 95.14%\n",
            "Batch 0: Loss = 0.1259\n",
            "Batch 100: Loss = 0.2501\n",
            "Batch 200: Loss = 0.1210\n",
            "Batch 300: Loss = 0.0368\n",
            "Batch 400: Loss = 0.0531\n",
            "Batch 500: Loss = 0.0823\n",
            "Batch 600: Loss = 0.0898\n",
            "Batch 700: Loss = 0.0913\n",
            "Batch 800: Loss = 0.1438\n",
            "Batch 900: Loss = 0.2638\n",
            "Batch 1000: Loss = 0.0563\n",
            "Batch 1100: Loss = 0.0892\n",
            "Batch 1200: Loss = 0.1530\n",
            "Batch 1300: Loss = 0.0822\n",
            "Batch 1400: Loss = 0.2660\n",
            "Batch 1500: Loss = 0.0948\n",
            "Batch 1600: Loss = 0.2865\n",
            "Batch 1700: Loss = 0.0680\n",
            "Batch 1800: Loss = 0.2021\n",
            "Epoch 46/50, Training Loss: 0.1781, Training Accuracy: 95.20%\n",
            "Batch 0: Loss = 0.0552\n",
            "Batch 100: Loss = 0.0996\n",
            "Batch 200: Loss = 0.2268\n",
            "Batch 300: Loss = 0.2010\n",
            "Batch 400: Loss = 0.4006\n",
            "Batch 500: Loss = 0.1876\n",
            "Batch 600: Loss = 0.1757\n",
            "Batch 700: Loss = 0.2083\n",
            "Batch 800: Loss = 0.0965\n",
            "Batch 900: Loss = 0.1668\n",
            "Batch 1000: Loss = 0.2843\n",
            "Batch 1100: Loss = 0.1310\n",
            "Batch 1200: Loss = 0.3079\n",
            "Batch 1300: Loss = 0.2323\n",
            "Batch 1400: Loss = 0.4261\n",
            "Batch 1500: Loss = 0.3174\n",
            "Batch 1600: Loss = 0.1336\n",
            "Batch 1700: Loss = 0.2173\n",
            "Batch 1800: Loss = 0.1480\n",
            "Epoch 47/50, Training Loss: 0.1759, Training Accuracy: 95.27%\n",
            "Batch 0: Loss = 0.1368\n",
            "Batch 100: Loss = 0.2009\n",
            "Batch 200: Loss = 0.4691\n",
            "Batch 300: Loss = 0.1635\n",
            "Batch 400: Loss = 0.2422\n",
            "Batch 500: Loss = 0.0573\n",
            "Batch 600: Loss = 0.0982\n",
            "Batch 700: Loss = 0.0520\n",
            "Batch 800: Loss = 0.3016\n",
            "Batch 900: Loss = 0.0906\n",
            "Batch 1000: Loss = 0.0649\n",
            "Batch 1100: Loss = 0.0683\n",
            "Batch 1200: Loss = 0.1932\n",
            "Batch 1300: Loss = 0.1058\n",
            "Batch 1400: Loss = 0.4945\n",
            "Batch 1500: Loss = 0.1116\n",
            "Batch 1600: Loss = 0.0474\n",
            "Batch 1700: Loss = 0.1913\n",
            "Batch 1800: Loss = 0.0817\n",
            "Epoch 48/50, Training Loss: 0.1737, Training Accuracy: 95.33%\n",
            "Batch 0: Loss = 0.0777\n",
            "Batch 100: Loss = 0.1878\n",
            "Batch 200: Loss = 0.1002\n",
            "Batch 300: Loss = 0.4433\n",
            "Batch 400: Loss = 0.1764\n",
            "Batch 500: Loss = 0.0938\n",
            "Batch 600: Loss = 0.2346\n",
            "Batch 700: Loss = 0.0891\n",
            "Batch 800: Loss = 0.1803\n",
            "Batch 900: Loss = 0.1303\n",
            "Batch 1000: Loss = 0.1520\n",
            "Batch 1100: Loss = 0.0439\n",
            "Batch 1200: Loss = 0.1127\n",
            "Batch 1300: Loss = 0.1966\n",
            "Batch 1400: Loss = 0.1843\n",
            "Batch 1500: Loss = 0.1773\n",
            "Batch 1600: Loss = 0.1576\n",
            "Batch 1700: Loss = 0.0949\n",
            "Batch 1800: Loss = 0.2376\n",
            "Epoch 49/50, Training Loss: 0.1716, Training Accuracy: 95.39%\n",
            "Batch 0: Loss = 0.1348\n",
            "Batch 100: Loss = 0.1012\n",
            "Batch 200: Loss = 0.0915\n",
            "Batch 300: Loss = 0.1348\n",
            "Batch 400: Loss = 0.1335\n",
            "Batch 500: Loss = 0.0779\n",
            "Batch 600: Loss = 0.1569\n",
            "Batch 700: Loss = 0.0950\n",
            "Batch 800: Loss = 0.1201\n",
            "Batch 900: Loss = 0.2082\n",
            "Batch 1000: Loss = 0.1519\n",
            "Batch 1100: Loss = 0.3096\n",
            "Batch 1200: Loss = 0.0885\n",
            "Batch 1300: Loss = 0.1383\n",
            "Batch 1400: Loss = 0.1786\n",
            "Batch 1500: Loss = 0.2343\n",
            "Batch 1600: Loss = 0.0809\n",
            "Batch 1700: Loss = 0.1409\n",
            "Batch 1800: Loss = 0.1840\n",
            "Epoch 50/50, Training Loss: 0.1695, Training Accuracy: 95.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing function\n",
        "def test_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images = images.view(images.shape[0], -1)  # Flatten images\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            correct = (predictions == labels).sum().item()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_corrects += correct\n",
        "            total_samples += labels.shape[0]\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    acc = (total_corrects / total_samples) * 100.0\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {acc:.2f}%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_model(model, test_loader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-_QUUY6FhxB",
        "outputId": "014da458-f3eb-4f0b-baef-17358d43f66a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1775, Test Accuracy: 94.95%\n"
          ]
        }
      ]
    }
  ]
}